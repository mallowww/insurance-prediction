{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mallowww/insurance-prediction/blob/main/MedicalCost_Prediction_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hH5O-71Q1jMS"
      },
      "source": [
        "## Insurance prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9sj2kdG1kS2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt  \n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeLp_Oxi1o4t"
      },
      "outputs": [],
      "source": [
        "URL=\"https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJg5SGFd1tuo"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeE4nbn33nhO"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "kvrACtgO1wkJ",
        "outputId": "dd5ddebe-6319-4928-a9db-3bcf1ac7e270"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c46b8ecd-f05c-45d3-ad5e-c1221af3552a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>50</td>\n",
              "      <td>male</td>\n",
              "      <td>30.970</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>10600.54830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>18</td>\n",
              "      <td>female</td>\n",
              "      <td>31.920</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northeast</td>\n",
              "      <td>2205.98080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>18</td>\n",
              "      <td>female</td>\n",
              "      <td>36.850</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1629.83350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>21</td>\n",
              "      <td>female</td>\n",
              "      <td>25.800</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southwest</td>\n",
              "      <td>2007.94500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>61</td>\n",
              "      <td>female</td>\n",
              "      <td>29.070</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>northwest</td>\n",
              "      <td>29141.36030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1338 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c46b8ecd-f05c-45d3-ad5e-c1221af3552a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c46b8ecd-f05c-45d3-ad5e-c1221af3552a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c46b8ecd-f05c-45d3-ad5e-c1221af3552a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      age     sex     bmi  children smoker     region      charges\n",
              "0      19  female  27.900         0    yes  southwest  16884.92400\n",
              "1      18    male  33.770         1     no  southeast   1725.55230\n",
              "2      28    male  33.000         3     no  southeast   4449.46200\n",
              "3      33    male  22.705         0     no  northwest  21984.47061\n",
              "4      32    male  28.880         0     no  northwest   3866.85520\n",
              "...   ...     ...     ...       ...    ...        ...          ...\n",
              "1333   50    male  30.970         3     no  northwest  10600.54830\n",
              "1334   18  female  31.920         0     no  northeast   2205.98080\n",
              "1335   18  female  36.850         0     no  southeast   1629.83350\n",
              "1336   21  female  25.800         0     no  southwest   2007.94500\n",
              "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
              "\n",
              "[1338 rows x 7 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyJW5uDn1yxK",
        "outputId": "a82535c1-23f8-49ff-fc68-e9bee1400165"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1338 entries, 0 to 1337\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1338 non-null   int64  \n",
            " 1   sex       1338 non-null   object \n",
            " 2   bmi       1338 non-null   float64\n",
            " 3   children  1338 non-null   int64  \n",
            " 4   smoker    1338 non-null   object \n",
            " 5   region    1338 non-null   object \n",
            " 6   charges   1338 non-null   float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 73.3+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oa1vxtgy11as",
        "outputId": "aa467676-f891-4013-85fc-c396a9d75b6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "bmi         0\n",
              "children    0\n",
              "smoker      0\n",
              "region      0\n",
              "charges     0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jg_RX5u2YLr",
        "outputId": "2e78604d-0d61-47c2-c091-c0fcccd31167"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjuKwC9I2c7h"
      },
      "outputs": [],
      "source": [
        "X = df.drop(['charges'], axis=1)\n",
        "y = df['charges']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9q2CA4Y2njE",
        "outputId": "ca76cc32-e9c6-4479-baa0-6f561cc6dbaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1338 entries, 0 to 1337\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1338 non-null   int64  \n",
            " 1   sex       1338 non-null   object \n",
            " 2   bmi       1338 non-null   float64\n",
            " 3   children  1338 non-null   int64  \n",
            " 4   smoker    1338 non-null   object \n",
            " 5   region    1338 non-null   object \n",
            "dtypes: float64(1), int64(2), object(3)\n",
            "memory usage: 62.8+ KB\n"
          ]
        }
      ],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vHMCwds2py_"
      },
      "source": [
        "Encode categorical features as an integer array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D1hIGf72qJS",
        "outputId": "ffef32a7-607d-428d-88bf-137934612dc6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['sex', 'smoker', 'region'], dtype='object')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categorical_X = X.select_dtypes(include='object')\n",
        "categorical_X.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4p9uhbE2wEA"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "enc = OrdinalEncoder()\n",
        "enc.fit(X[categorical_X.columns])\n",
        "X[categorical_X.columns] = enc.transform(X[categorical_X.columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5mHI4Odq2xYv",
        "outputId": "996e4b80-60b8-4087-8488-41962ef83793"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a6766102-547a-4dd3-a0f3-c4ef7d42af2b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>1.0</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>1.0</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>1.0</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>1.0</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>30.970</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.920</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.850</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>21</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.800</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>61</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29.070</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1338 rows Ã— 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a6766102-547a-4dd3-a0f3-c4ef7d42af2b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a6766102-547a-4dd3-a0f3-c4ef7d42af2b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a6766102-547a-4dd3-a0f3-c4ef7d42af2b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      age  sex     bmi  children  smoker  region\n",
              "0      19  0.0  27.900         0     1.0     3.0\n",
              "1      18  1.0  33.770         1     0.0     2.0\n",
              "2      28  1.0  33.000         3     0.0     2.0\n",
              "3      33  1.0  22.705         0     0.0     1.0\n",
              "4      32  1.0  28.880         0     0.0     1.0\n",
              "...   ...  ...     ...       ...     ...     ...\n",
              "1333   50  1.0  30.970         3     0.0     1.0\n",
              "1334   18  0.0  31.920         0     0.0     0.0\n",
              "1335   18  0.0  36.850         0     0.0     2.0\n",
              "1336   21  0.0  25.800         0     0.0     3.0\n",
              "1337   61  0.0  29.070         0     1.0     1.0\n",
              "\n",
              "[1338 rows x 6 columns]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ED5YO-f21wv",
        "outputId": "1697211e-55c4-422a-96aa-9bcb35f0a021"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "age         0\n",
              "sex         0\n",
              "bmi         0\n",
              "children    0\n",
              "smoker      0\n",
              "region      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fK-TTRaa26n4"
      },
      "source": [
        "Handle Imbalanced Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuT_-JEy29uX",
        "outputId": "1432b3aa-f2fe-4223-945c-d7a66ceb4d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1.12187390e+03 1.00000000e+00]\n",
            " [1.13150660e+03 1.00000000e+00]\n",
            " [1.13594070e+03 1.00000000e+00]\n",
            " ...\n",
            " [6.00213990e+04 1.00000000e+00]\n",
            " [6.25928731e+04 1.00000000e+00]\n",
            " [6.37704280e+04 1.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "unique_elements, counts_elements = np.unique(y, return_counts=True)\n",
        "print(np.asarray((unique_elements, counts_elements)).T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cE2j3h8j3LFO",
        "outputId": "8c8532ab-0697-4d2b-baef-9adbd75bd223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1338 entries, 0 to 1337\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1338 non-null   int64  \n",
            " 1   sex       1338 non-null   float64\n",
            " 2   bmi       1338 non-null   float64\n",
            " 3   children  1338 non-null   int64  \n",
            " 4   smoker    1338 non-null   float64\n",
            " 5   region    1338 non-null   float64\n",
            "dtypes: float64(4), int64(2)\n",
            "memory usage: 62.8 KB\n"
          ]
        }
      ],
      "source": [
        "X.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kkCPOCW3Uox"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training set and test set 70% training and 30% test\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=104)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ex9alCe3bC_"
      },
      "source": [
        "Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUHwf00b3doy"
      },
      "outputs": [],
      "source": [
        "# Standardization, or mean removal and variance scaling\n",
        "# Standardization of datasets is a common requirement for many machine learning\n",
        "# estimators implemented in scikit-learn; they might behave badly if the \n",
        "# individual features do not more or less look like standard normally distributed data: \n",
        "# Gaussian with zero mean and unit variance.\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "# Fit only to the training data\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Now apply the transformations to the data:\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s-5Q0eM3fUu",
        "outputId": "5f10a8c8-4fc4-4cb1-85ce-0d16d8005a42"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.07946364,  0.99573559, -1.84722724, -0.91727834,  1.97497432,\n",
              "         1.34378726],\n",
              "       [ 1.50304626,  0.99573559, -1.02819505, -0.91727834, -0.5063357 ,\n",
              "        -0.45433265],\n",
              "       [ 1.431114  ,  0.99573559, -0.84219969, -0.91727834, -0.5063357 ,\n",
              "        -0.45433265],\n",
              "       ...,\n",
              "       [ 1.28724946,  0.99573559, -1.12935042, -0.91727834, -0.5063357 ,\n",
              "         1.34378726],\n",
              "       [ 0.13633317, -1.00428267,  0.89049392, -0.08613359, -0.5063357 ,\n",
              "         0.44472731],\n",
              "       [-0.79878632,  0.99573559, -1.08366735,  1.57615591,  1.97497432,\n",
              "         0.44472731]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThmEM5Yj3qGv"
      },
      "source": [
        "## Choosing the right estimator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3k_yPTG4Pg8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNqxmKhC6NCr"
      },
      "source": [
        "LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnWfnr946Kk",
        "outputId": "1e55e069-67f3-48d3-e218-af046786f51a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 31720801.346055053\n",
            "R2 score: 0.7783016183871759\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPf0SyDM6fFc"
      },
      "source": [
        "ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-wcof406N0s",
        "outputId": "e302e2e6-9def-4257-c2eb-2ae9257de0ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 86014273.28871724\n",
            "R2 score: 0.39884163153137653\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = ElasticNet()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERjwkRKR6fbJ"
      },
      "source": [
        "Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dThWf0al6o5w",
        "outputId": "94d0e23f-efd3-4e69-c988-436be4e64488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 31717524.758911017\n",
            "R2 score: 0.7783245186304303\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = Lasso()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIKuMr7J66_G"
      },
      "source": [
        "SVR (kernel='linear')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X49XpqNh68Xt",
        "outputId": "09725780-63f7-4024-d8fb-dbbbb11d848b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 166760393.56031412\n",
            "R2 score: -0.16549733299966163\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = SVR(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZKCeKba6_VT"
      },
      "source": [
        "SVR (kernel='rbf'):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9vHMKTA6-5y",
        "outputId": "806dca5e-0445-4599-a233-a2cb8f0602b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 157320307.15834376\n",
            "R2 score: -0.09952006291842097\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = SVR(kernel='rbf')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnM_RN-97DOy"
      },
      "source": [
        "EnsembleRegressor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE45Q1M37Di5",
        "outputId": "522bc273-3746-4492-f784-8a06553df2a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 18070964.104027204\n",
            "R2 score: 0.873701062834451\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import VotingRegressor\n",
        "\n",
        "# create individual models\n",
        "model1 = RandomForestRegressor()\n",
        "model2 = AdaBoostRegressor()\n",
        "model3 = GradientBoostingRegressor()\n",
        "\n",
        "# create ensemble model\n",
        "ensemble_model = VotingRegressor(estimators=[('rf', model1), ('ada', model2), ('gb', model3)])\n",
        "\n",
        "# fit the model\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = ensemble_model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab893c4y7MpZ"
      },
      "source": [
        "Ridge Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5TEwLe57PyA",
        "outputId": "64838785-67c5-4aa2-e69e-3a86c494a684"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean squared error: 31684079.761372153\n",
            "R2 score: 0.7785582675116934\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4qX-3FXG8jx"
      },
      "source": [
        "MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXc-lWv5G-br",
        "outputId": "4fdf6618-f618-47ff-ded8-ad9e32434695"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 162282053.84390649\n",
            "Iteration 2, loss = 162142853.16770035\n",
            "Iteration 3, loss = 162015470.36818638\n",
            "Iteration 4, loss = 161881611.32208744\n",
            "Iteration 5, loss = 161732604.70378876\n",
            "Iteration 6, loss = 161558318.24786866\n",
            "Iteration 7, loss = 161357191.22129709\n",
            "Iteration 8, loss = 161114857.16129392\n",
            "Iteration 9, loss = 160826875.97026318\n",
            "Iteration 10, loss = 160474404.74140573\n",
            "Iteration 11, loss = 160058244.37816665\n",
            "Iteration 12, loss = 159569873.04883951\n",
            "Iteration 13, loss = 158981119.36753285\n",
            "Iteration 14, loss = 158302508.33471650\n",
            "Iteration 15, loss = 157525871.15469831\n",
            "Iteration 16, loss = 156634682.13688010\n",
            "Iteration 17, loss = 155587877.12772968\n",
            "Iteration 18, loss = 154432268.76683149\n",
            "Iteration 19, loss = 153105634.89138255\n",
            "Iteration 20, loss = 151638539.25354871\n",
            "Iteration 21, loss = 149983406.48173210\n",
            "Iteration 22, loss = 148145005.80860263\n",
            "Iteration 23, loss = 146202535.60051629\n",
            "Iteration 24, loss = 144036946.66518158\n",
            "Iteration 25, loss = 141689203.18733022\n",
            "Iteration 26, loss = 139173641.32884914\n",
            "Iteration 27, loss = 136499567.63280222\n",
            "Iteration 28, loss = 133652966.57188578\n",
            "Iteration 29, loss = 130594547.57501690\n",
            "Iteration 30, loss = 127485239.03513424\n",
            "Iteration 31, loss = 124232868.16968369\n",
            "Iteration 32, loss = 120784298.00477566\n",
            "Iteration 33, loss = 117335014.66404934\n",
            "Iteration 34, loss = 113765600.72461072\n",
            "Iteration 35, loss = 110101389.55643526\n",
            "Iteration 36, loss = 106562297.11956178\n",
            "Iteration 37, loss = 102996027.42113645\n",
            "Iteration 38, loss = 99363551.79102352\n",
            "Iteration 39, loss = 95886243.18430234\n",
            "Iteration 40, loss = 92535589.56720704\n",
            "Iteration 41, loss = 89360908.64765750\n",
            "Iteration 42, loss = 86166285.53474581\n",
            "Iteration 43, loss = 83302791.87911981\n",
            "Iteration 44, loss = 80553365.06436402\n",
            "Iteration 45, loss = 78196289.54795989\n",
            "Iteration 46, loss = 75869988.64014986\n",
            "Iteration 47, loss = 74004756.64343232\n",
            "Iteration 48, loss = 72181617.61561534\n",
            "Iteration 49, loss = 70659674.61567061\n",
            "Iteration 50, loss = 69505751.49905878\n",
            "Iteration 51, loss = 68415049.57098261\n",
            "Iteration 52, loss = 67616213.08988710\n",
            "Iteration 53, loss = 66909836.34210386\n",
            "Iteration 54, loss = 66405699.06183952\n",
            "Iteration 55, loss = 66022366.94695787\n",
            "Iteration 56, loss = 65755574.22403000\n",
            "Iteration 57, loss = 65530315.45517477\n",
            "Iteration 58, loss = 65365101.53393175\n",
            "Iteration 59, loss = 65297222.56940827\n",
            "Iteration 60, loss = 65199230.51602207\n",
            "Iteration 61, loss = 65155313.83546221\n",
            "Iteration 62, loss = 65120988.49282983\n",
            "Iteration 63, loss = 65097150.44515511\n",
            "Iteration 64, loss = 65088008.27857508\n",
            "Iteration 65, loss = 65068594.85004865\n",
            "Iteration 66, loss = 65051185.89772121\n",
            "Iteration 67, loss = 65040280.40269341\n",
            "Iteration 68, loss = 65026869.23423451\n",
            "Iteration 69, loss = 65012893.57143756\n",
            "Iteration 70, loss = 65001748.68888627\n",
            "Iteration 71, loss = 64991059.14201382\n",
            "Iteration 72, loss = 64979525.79177655\n",
            "Iteration 73, loss = 64963005.31152798\n",
            "Iteration 74, loss = 64952760.78920674\n",
            "Iteration 75, loss = 64938945.78713655\n",
            "Iteration 76, loss = 64929628.89376631\n",
            "Iteration 77, loss = 64918721.83099423\n",
            "Iteration 78, loss = 64904372.86647595\n",
            "Iteration 79, loss = 64891851.70661959\n",
            "Iteration 80, loss = 64880289.76316547\n",
            "Iteration 81, loss = 64866193.82894877\n",
            "Iteration 82, loss = 64858474.22550327\n",
            "Iteration 83, loss = 64842537.48971641\n",
            "Iteration 84, loss = 64830064.05960532\n",
            "Iteration 85, loss = 64820201.07549014\n",
            "Iteration 86, loss = 64806635.82833351\n",
            "Iteration 87, loss = 64796686.50448904\n",
            "Iteration 88, loss = 64781722.93372467\n",
            "Iteration 89, loss = 64777474.86640236\n",
            "Iteration 90, loss = 64758435.57268604\n",
            "Iteration 91, loss = 64747490.23931649\n",
            "Iteration 92, loss = 64734074.33847144\n",
            "Iteration 93, loss = 64722315.62579489\n",
            "Iteration 94, loss = 64712255.19825371\n",
            "Iteration 95, loss = 64703023.01565249\n",
            "Iteration 96, loss = 64688664.92139448\n",
            "Iteration 97, loss = 64676699.27322536\n",
            "Iteration 98, loss = 64662787.98588957\n",
            "Iteration 99, loss = 64657081.69520740\n",
            "Iteration 100, loss = 64643020.58012751\n",
            "Iteration 101, loss = 64629472.90714984\n",
            "Iteration 102, loss = 64616158.16228811\n",
            "Iteration 103, loss = 64608429.21886682\n",
            "Iteration 104, loss = 64593412.15996327\n",
            "Iteration 105, loss = 64580616.92947174\n",
            "Iteration 106, loss = 64568371.17855608\n",
            "Iteration 107, loss = 64556772.23860746\n",
            "Iteration 108, loss = 64545840.09074180\n",
            "Iteration 109, loss = 64532956.21522835\n",
            "Iteration 110, loss = 64521461.62296328\n",
            "Iteration 111, loss = 64511744.04254846\n",
            "Iteration 112, loss = 64497407.52032126\n",
            "Iteration 113, loss = 64491643.29810333\n",
            "Iteration 114, loss = 64473767.85568245\n",
            "Iteration 115, loss = 64463095.67524483\n",
            "Iteration 116, loss = 64450431.04338634\n",
            "Iteration 117, loss = 64437851.79201490\n",
            "Iteration 118, loss = 64431602.13089870\n",
            "Iteration 119, loss = 64414333.77955464\n",
            "Iteration 120, loss = 64404033.91964897\n",
            "Iteration 121, loss = 64390908.51733855\n",
            "Iteration 122, loss = 64377961.46298048\n",
            "Iteration 123, loss = 64367917.00696650\n",
            "Iteration 124, loss = 64355561.93137564\n",
            "Iteration 125, loss = 64342567.89721461\n",
            "Iteration 126, loss = 64330943.21499866\n",
            "Iteration 127, loss = 64320836.51450852\n",
            "Iteration 128, loss = 64307582.81873269\n",
            "Iteration 129, loss = 64293411.96349691\n",
            "Iteration 130, loss = 64281737.10958865\n",
            "Iteration 131, loss = 64268984.04087840\n",
            "Iteration 132, loss = 64255909.75190198\n",
            "Iteration 133, loss = 64247545.35573733\n",
            "Iteration 134, loss = 64231015.99735964\n",
            "Iteration 135, loss = 64224018.06987204\n",
            "Iteration 136, loss = 64206920.58517587\n",
            "Iteration 137, loss = 64193841.59985496\n",
            "Iteration 138, loss = 64182174.83904839\n",
            "Iteration 139, loss = 64173740.54571237\n",
            "Iteration 140, loss = 64157428.27645721\n",
            "Iteration 141, loss = 64143100.97349136\n",
            "Iteration 142, loss = 64135204.28293698\n",
            "Iteration 143, loss = 64117870.57943545\n",
            "Iteration 144, loss = 64105274.61821806\n",
            "Iteration 145, loss = 64090297.04591013\n",
            "Iteration 146, loss = 64079870.12374260\n",
            "Iteration 147, loss = 64064571.82600271\n",
            "Iteration 148, loss = 64053358.23220371\n",
            "Iteration 149, loss = 64037233.60279562\n",
            "Iteration 150, loss = 64029199.34569551\n",
            "Iteration 151, loss = 64012932.65375242\n",
            "Iteration 152, loss = 63998055.20371427\n",
            "Iteration 153, loss = 63987235.90981372\n",
            "Iteration 154, loss = 63971676.83706174\n",
            "Iteration 155, loss = 63960364.30851731\n",
            "Iteration 156, loss = 63946115.25642092\n",
            "Iteration 157, loss = 63932435.98714010\n",
            "Iteration 158, loss = 63916877.57920723\n",
            "Iteration 159, loss = 63908124.36188392\n",
            "Iteration 160, loss = 63891595.56308080\n",
            "Iteration 161, loss = 63876163.26192628\n",
            "Iteration 162, loss = 63861762.03561280\n",
            "Iteration 163, loss = 63857394.78014212\n",
            "Iteration 164, loss = 63842491.63260891\n",
            "Iteration 165, loss = 63821977.73984379\n",
            "Iteration 166, loss = 63809726.60928098\n",
            "Iteration 167, loss = 63794983.48515462\n",
            "Iteration 168, loss = 63786457.63646728\n",
            "Iteration 169, loss = 63767229.87140000\n",
            "Iteration 170, loss = 63752801.05195222\n",
            "Iteration 171, loss = 63741321.11763761\n",
            "Iteration 172, loss = 63722175.89076906\n",
            "Iteration 173, loss = 63709237.87464755\n",
            "Iteration 174, loss = 63696202.79384396\n",
            "Iteration 175, loss = 63680177.91437788\n",
            "Iteration 176, loss = 63667527.29546043\n",
            "Iteration 177, loss = 63661790.39037620\n",
            "Iteration 178, loss = 63638951.85452349\n",
            "Iteration 179, loss = 63624033.32792006\n",
            "Iteration 180, loss = 63611213.61214065\n",
            "Iteration 181, loss = 63599857.21595429\n",
            "Iteration 182, loss = 63584117.07411280\n",
            "Iteration 183, loss = 63567588.47764553\n",
            "Iteration 184, loss = 63553264.57894669\n",
            "Iteration 185, loss = 63539252.13579407\n",
            "Iteration 186, loss = 63538868.20502731\n",
            "Iteration 187, loss = 63506166.22781093\n",
            "Iteration 188, loss = 63491228.67648984\n",
            "Iteration 189, loss = 63485390.21930566\n",
            "Iteration 190, loss = 63468305.31751876\n",
            "Iteration 191, loss = 63455438.75976198\n",
            "Iteration 192, loss = 63436229.81499360\n",
            "Iteration 193, loss = 63419111.43344410\n",
            "Iteration 194, loss = 63409746.97679267\n",
            "Iteration 195, loss = 63393963.38640282\n",
            "Iteration 196, loss = 63377620.42460278\n",
            "Iteration 197, loss = 63360632.33018757\n",
            "Iteration 198, loss = 63352036.50333083\n",
            "Iteration 199, loss = 63329472.06568348\n",
            "Iteration 200, loss = 63314623.59364100\n",
            "Iteration 201, loss = 63311664.31628820\n",
            "Iteration 202, loss = 63284401.90538843\n",
            "Iteration 203, loss = 63269410.93634158\n",
            "Iteration 204, loss = 63252039.47178798\n",
            "Iteration 205, loss = 63238329.74391685\n",
            "Iteration 206, loss = 63221480.36505084\n",
            "Iteration 207, loss = 63213519.74669932\n",
            "Iteration 208, loss = 63199222.86473584\n",
            "Iteration 209, loss = 63182966.43080296\n",
            "Iteration 210, loss = 63159930.81658258\n",
            "Iteration 211, loss = 63145364.76878998\n",
            "Iteration 212, loss = 63129844.42636159\n",
            "Iteration 213, loss = 63115063.14658512\n",
            "Iteration 214, loss = 63098550.64512327\n",
            "Iteration 215, loss = 63083995.82858171\n",
            "Iteration 216, loss = 63062795.50356658\n",
            "Iteration 217, loss = 63042800.01905911\n",
            "Iteration 218, loss = 63030797.67357645\n",
            "Iteration 219, loss = 63011508.58130025\n",
            "Iteration 220, loss = 62995921.98209645\n",
            "Iteration 221, loss = 62979507.00289400\n",
            "Iteration 222, loss = 62959745.10328489\n",
            "Iteration 223, loss = 62941409.38692983\n",
            "Iteration 224, loss = 62929007.73659918\n",
            "Iteration 225, loss = 62908020.26041585\n",
            "Iteration 226, loss = 62892318.25000576\n",
            "Iteration 227, loss = 62872172.31080766\n",
            "Iteration 228, loss = 62860747.20800836\n",
            "Iteration 229, loss = 62841067.15682023\n",
            "Iteration 230, loss = 62819417.49861649\n",
            "Iteration 231, loss = 62801383.77162767\n",
            "Iteration 232, loss = 62782689.56808848\n",
            "Iteration 233, loss = 62766377.98927003\n",
            "Iteration 234, loss = 62747834.56170724\n",
            "Iteration 235, loss = 62732043.40963251\n",
            "Iteration 236, loss = 62716471.36506885\n",
            "Iteration 237, loss = 62705139.09793250\n",
            "Iteration 238, loss = 62672700.88378149\n",
            "Iteration 239, loss = 62663134.87772698\n",
            "Iteration 240, loss = 62638202.81412847\n",
            "Iteration 241, loss = 62616877.06229221\n",
            "Iteration 242, loss = 62609528.38047242\n",
            "Iteration 243, loss = 62582499.51406439\n",
            "Iteration 244, loss = 62565361.19808619\n",
            "Iteration 245, loss = 62542468.78671960\n",
            "Iteration 246, loss = 62532071.71663104\n",
            "Iteration 247, loss = 62499944.80336931\n",
            "Iteration 248, loss = 62485726.78245535\n",
            "Iteration 249, loss = 62471568.69840185\n",
            "Iteration 250, loss = 62443942.28451331\n",
            "Iteration 251, loss = 62431234.31703534\n",
            "Iteration 252, loss = 62410225.82915080\n",
            "Iteration 253, loss = 62389949.39430173\n",
            "Iteration 254, loss = 62368125.92150860\n",
            "Iteration 255, loss = 62346280.60251206\n",
            "Iteration 256, loss = 62334014.01441506\n",
            "Iteration 257, loss = 62306990.70343605\n",
            "Iteration 258, loss = 62285442.93610553\n",
            "Iteration 259, loss = 62266453.06520317\n",
            "Iteration 260, loss = 62245909.64389056\n",
            "Iteration 261, loss = 62226980.26373003\n",
            "Iteration 262, loss = 62227599.81903055\n",
            "Iteration 263, loss = 62182932.63953007\n",
            "Iteration 264, loss = 62169077.39391159\n",
            "Iteration 265, loss = 62140661.19344010\n",
            "Iteration 266, loss = 62121332.59302162\n",
            "Iteration 267, loss = 62099638.46334819\n",
            "Iteration 268, loss = 62075541.80561845\n",
            "Iteration 269, loss = 62055091.68421497\n",
            "Iteration 270, loss = 62041852.07218765\n",
            "Iteration 271, loss = 62016316.64592025\n",
            "Iteration 272, loss = 61994161.91409636\n",
            "Iteration 273, loss = 61972024.61698548\n",
            "Iteration 274, loss = 61945697.75546566\n",
            "Iteration 275, loss = 61929366.54772066\n",
            "Iteration 276, loss = 61901199.70906600\n",
            "Iteration 277, loss = 61880398.20619779\n",
            "Iteration 278, loss = 61867596.44355708\n",
            "Iteration 279, loss = 61832686.95250925\n",
            "Iteration 280, loss = 61812387.38283089\n",
            "Iteration 281, loss = 61788029.16934698\n",
            "Iteration 282, loss = 61770916.45270823\n",
            "Iteration 283, loss = 61747347.96817538\n",
            "Iteration 284, loss = 61723349.45186146\n",
            "Iteration 285, loss = 61703515.86767260\n",
            "Iteration 286, loss = 61671315.59754566\n",
            "Iteration 287, loss = 61648927.12162220\n",
            "Iteration 288, loss = 61631204.29359300\n",
            "Iteration 289, loss = 61605011.71327077\n",
            "Iteration 290, loss = 61575270.98749088\n",
            "Iteration 291, loss = 61552155.86117065\n",
            "Iteration 292, loss = 61524655.63762379\n",
            "Iteration 293, loss = 61500274.32946964\n",
            "Iteration 294, loss = 61485804.65357900\n",
            "Iteration 295, loss = 61454388.81475298\n",
            "Iteration 296, loss = 61426000.49185438\n",
            "Iteration 297, loss = 61401655.47669783\n",
            "Iteration 298, loss = 61392409.31542347\n",
            "Iteration 299, loss = 61355455.26821691\n",
            "Iteration 300, loss = 61330883.86249619\n",
            "Iteration 301, loss = 61298350.99704507\n",
            "Iteration 302, loss = 61286401.09663507\n",
            "Iteration 303, loss = 61246147.70497025\n",
            "Iteration 304, loss = 61220913.28190180\n",
            "Iteration 305, loss = 61199489.30567028\n",
            "Iteration 306, loss = 61173954.89043634\n",
            "Iteration 307, loss = 61143597.58281652\n",
            "Iteration 308, loss = 61114550.56044144\n",
            "Iteration 309, loss = 61102925.99913110\n",
            "Iteration 310, loss = 61063758.19850966\n",
            "Iteration 311, loss = 61034564.42749212\n",
            "Iteration 312, loss = 61007160.12397854\n",
            "Iteration 313, loss = 61006285.49539681\n",
            "Iteration 314, loss = 60951917.45809482\n",
            "Iteration 315, loss = 60924094.94929852\n",
            "Iteration 316, loss = 60892918.37641682\n",
            "Iteration 317, loss = 60880397.54990663\n",
            "Iteration 318, loss = 60839784.25151052\n",
            "Iteration 319, loss = 60815530.04501799\n",
            "Iteration 320, loss = 60783395.28788424\n",
            "Iteration 321, loss = 60756689.76838659\n",
            "Iteration 322, loss = 60725222.44240824\n",
            "Iteration 323, loss = 60700489.34319060\n",
            "Iteration 324, loss = 60668699.87386109\n",
            "Iteration 325, loss = 60636427.70285713\n",
            "Iteration 326, loss = 60601578.51432499\n",
            "Iteration 327, loss = 60567425.31001119\n",
            "Iteration 328, loss = 60540033.60001720\n",
            "Iteration 329, loss = 60517458.81025332\n",
            "Iteration 330, loss = 60480252.05732868\n",
            "Iteration 331, loss = 60449808.54961207\n",
            "Iteration 332, loss = 60418879.40359376\n",
            "Iteration 333, loss = 60387664.79249541\n",
            "Iteration 334, loss = 60367727.41360649\n",
            "Iteration 335, loss = 60323637.21131421\n",
            "Iteration 336, loss = 60291640.89220211\n",
            "Iteration 337, loss = 60258367.89336351\n",
            "Iteration 338, loss = 60226468.26790633\n",
            "Iteration 339, loss = 60190708.36998133\n",
            "Iteration 340, loss = 60164374.68102667\n",
            "Iteration 341, loss = 60119103.14008905\n",
            "Iteration 342, loss = 60096835.57615463\n",
            "Iteration 343, loss = 60064256.07034960\n",
            "Iteration 344, loss = 60026197.78080086\n",
            "Iteration 345, loss = 59997136.01987650\n",
            "Iteration 346, loss = 59952538.05585880\n",
            "Iteration 347, loss = 59918259.59896597\n",
            "Iteration 348, loss = 59882512.34872837\n",
            "Iteration 349, loss = 59845452.72533496\n",
            "Iteration 350, loss = 59806443.51537204\n",
            "Iteration 351, loss = 59773435.18203120\n",
            "Iteration 352, loss = 59746692.83788033\n",
            "Iteration 353, loss = 59699551.59146594\n",
            "Iteration 354, loss = 59664740.42676686\n",
            "Iteration 355, loss = 59627742.63894761\n",
            "Iteration 356, loss = 59593365.85152669\n",
            "Iteration 357, loss = 59552891.00345949\n",
            "Iteration 358, loss = 59528420.87426564\n",
            "Iteration 359, loss = 59488366.93106677\n",
            "Iteration 360, loss = 59432492.01756012\n",
            "Iteration 361, loss = 59402143.71473289\n",
            "Iteration 362, loss = 59438324.04849523\n",
            "Iteration 363, loss = 59328559.62835495\n",
            "Iteration 364, loss = 59289441.35093221\n",
            "Iteration 365, loss = 59235508.72290716\n",
            "Iteration 366, loss = 59221493.97860556\n",
            "Iteration 367, loss = 59160695.92841241\n",
            "Iteration 368, loss = 59113435.94764259\n",
            "Iteration 369, loss = 59071430.29632497\n",
            "Iteration 370, loss = 59040138.09064385\n",
            "Iteration 371, loss = 59000698.72841191\n",
            "Iteration 372, loss = 58956333.29049679\n",
            "Iteration 373, loss = 58910419.97935337\n",
            "Iteration 374, loss = 58871279.02199270\n",
            "Iteration 375, loss = 58821323.65534727\n",
            "Iteration 376, loss = 58779098.70305131\n",
            "Iteration 377, loss = 58732744.88129480\n",
            "Iteration 378, loss = 58689644.07002362\n",
            "Iteration 379, loss = 58640681.59692593\n",
            "Iteration 380, loss = 58610872.98326616\n",
            "Iteration 381, loss = 58550620.39497627\n",
            "Iteration 382, loss = 58520910.04299322\n",
            "Iteration 383, loss = 58466514.24026191\n",
            "Iteration 384, loss = 58415688.28578682\n",
            "Iteration 385, loss = 58370077.04672197\n",
            "Iteration 386, loss = 58332226.49146926\n",
            "Iteration 387, loss = 58272172.98521153\n",
            "Iteration 388, loss = 58226214.02181319\n",
            "Iteration 389, loss = 58178532.03650054\n",
            "Iteration 390, loss = 58124807.42221498\n",
            "Iteration 391, loss = 58134152.51203009\n",
            "Iteration 392, loss = 58028346.95994712\n",
            "Iteration 393, loss = 57986400.85758974\n",
            "Iteration 394, loss = 57935272.11042941\n",
            "Iteration 395, loss = 57895288.11375291\n",
            "Iteration 396, loss = 57842793.04516643\n",
            "Iteration 397, loss = 57791611.62165011\n",
            "Iteration 398, loss = 57722731.07520042\n",
            "Iteration 399, loss = 57681973.24255323\n",
            "Iteration 400, loss = 57623078.90396851\n",
            "Iteration 401, loss = 57561220.05919259\n",
            "Iteration 402, loss = 57507040.09136253\n",
            "Iteration 403, loss = 57463830.49572443\n",
            "Iteration 404, loss = 57408595.38329955\n",
            "Iteration 405, loss = 57345634.61912763\n",
            "Iteration 406, loss = 57290506.79214396\n",
            "Iteration 407, loss = 57234391.85548893\n",
            "Iteration 408, loss = 57172923.58739893\n",
            "Iteration 409, loss = 57128902.41079074\n",
            "Iteration 410, loss = 57064693.88756612\n",
            "Iteration 411, loss = 57005611.51862116\n",
            "Iteration 412, loss = 56943318.21414530\n",
            "Iteration 413, loss = 56885304.70125188\n",
            "Iteration 414, loss = 56825851.09991309\n",
            "Iteration 415, loss = 56768488.81159756\n",
            "Iteration 416, loss = 56705484.07595047\n",
            "Iteration 417, loss = 56638473.34538396\n",
            "Iteration 418, loss = 56580322.25457632\n",
            "Iteration 419, loss = 56531132.76914717\n",
            "Iteration 420, loss = 56449863.66623323\n",
            "Iteration 421, loss = 56394435.75741010\n",
            "Iteration 422, loss = 56328730.43509164\n",
            "Iteration 423, loss = 56267127.49407797\n",
            "Iteration 424, loss = 56197461.89512355\n",
            "Iteration 425, loss = 56124261.91252676\n",
            "Iteration 426, loss = 56063434.52482233\n",
            "Iteration 427, loss = 55985728.76430605\n",
            "Iteration 428, loss = 55912860.59177025\n",
            "Iteration 429, loss = 55844114.99757360\n",
            "Iteration 430, loss = 55773909.50111627\n",
            "Iteration 431, loss = 55718778.89090703\n",
            "Iteration 432, loss = 55633972.72711424\n",
            "Iteration 433, loss = 55563541.68231230\n",
            "Iteration 434, loss = 55498883.14733866\n",
            "Iteration 435, loss = 55418249.66518684\n",
            "Iteration 436, loss = 55353789.03903544\n",
            "Iteration 437, loss = 55278291.82586313\n",
            "Iteration 438, loss = 55192974.54347510\n",
            "Iteration 439, loss = 55112275.17084920\n",
            "Iteration 440, loss = 55037492.91327153\n",
            "Iteration 441, loss = 54954258.75326163\n",
            "Iteration 442, loss = 54881829.00572346\n",
            "Iteration 443, loss = 54824756.67988908\n",
            "Iteration 444, loss = 54727753.54810677\n",
            "Iteration 445, loss = 54639354.04090200\n",
            "Iteration 446, loss = 54563201.60272605\n",
            "Iteration 447, loss = 54494609.71733676\n",
            "Iteration 448, loss = 54386130.99145515\n",
            "Iteration 449, loss = 54306947.88914970\n",
            "Iteration 450, loss = 54249806.72121596\n",
            "Iteration 451, loss = 54131754.53200375\n",
            "Iteration 452, loss = 54073609.43061614\n",
            "Iteration 453, loss = 53975539.55258641\n",
            "Iteration 454, loss = 53891301.48300190\n",
            "Iteration 455, loss = 53795014.42913993\n",
            "Iteration 456, loss = 53698282.36505385\n",
            "Iteration 457, loss = 53612022.60025372\n",
            "Iteration 458, loss = 53529610.13849681\n",
            "Iteration 459, loss = 53433651.23060184\n",
            "Iteration 460, loss = 53369132.55402146\n",
            "Iteration 461, loss = 53255340.19074555\n",
            "Iteration 462, loss = 53150934.45158010\n",
            "Iteration 463, loss = 53065980.27127656\n",
            "Iteration 464, loss = 52963081.15341777\n",
            "Iteration 465, loss = 52866812.67207253\n",
            "Iteration 466, loss = 52784664.87112427\n",
            "Iteration 467, loss = 52681164.82086135\n",
            "Iteration 468, loss = 52595703.66322429\n",
            "Iteration 469, loss = 52482357.48348825\n",
            "Iteration 470, loss = 52379576.90618277\n",
            "Iteration 471, loss = 52281596.80415055\n",
            "Iteration 472, loss = 52175305.03061496\n",
            "Iteration 473, loss = 52067429.53545810\n",
            "Iteration 474, loss = 51961204.83550932\n",
            "Iteration 475, loss = 51862356.19132646\n",
            "Iteration 476, loss = 51752947.95013589\n",
            "Iteration 477, loss = 51643801.83741599\n",
            "Iteration 478, loss = 51530960.47034801\n",
            "Iteration 479, loss = 51446911.18067906\n",
            "Iteration 480, loss = 51324321.23946346\n",
            "Iteration 481, loss = 51210500.48051347\n",
            "Iteration 482, loss = 51101844.51048966\n",
            "Iteration 483, loss = 50984177.75131588\n",
            "Iteration 484, loss = 50873834.27293220\n",
            "Iteration 485, loss = 50765994.75688368\n",
            "Iteration 486, loss = 50638521.79380804\n",
            "Iteration 487, loss = 50522899.71122005\n",
            "Iteration 488, loss = 50424304.71875138\n",
            "Iteration 489, loss = 50340707.37596437\n",
            "Iteration 490, loss = 50172948.85806857\n",
            "Iteration 491, loss = 50062233.77544890\n",
            "Iteration 492, loss = 49934672.73765372\n",
            "Iteration 493, loss = 49809828.44613405\n",
            "Iteration 494, loss = 49689139.02330508\n",
            "Iteration 495, loss = 49569076.49103626\n",
            "Iteration 496, loss = 49440407.83242743\n",
            "Iteration 497, loss = 49349839.06192869\n",
            "Iteration 498, loss = 49195598.35470522\n",
            "Iteration 499, loss = 49064277.56811001\n",
            "Iteration 500, loss = 48937577.74689076\n",
            "Iteration 501, loss = 48810718.38390198\n",
            "Iteration 502, loss = 48688153.20666694\n",
            "Iteration 503, loss = 48582018.12034398\n",
            "Iteration 504, loss = 48427060.50255913\n",
            "Iteration 505, loss = 48280189.97930490\n",
            "Iteration 506, loss = 48158476.42839789\n",
            "Iteration 507, loss = 48027434.41131245\n",
            "Iteration 508, loss = 47887728.46213758\n",
            "Iteration 509, loss = 47740545.50478441\n",
            "Iteration 510, loss = 47610017.40808576\n",
            "Iteration 511, loss = 47463314.88992186\n",
            "Iteration 512, loss = 47339814.49459894\n",
            "Iteration 513, loss = 47199916.24987722\n",
            "Iteration 514, loss = 47054031.54957137\n",
            "Iteration 515, loss = 46946756.44616152\n",
            "Iteration 516, loss = 46760438.70481677\n",
            "Iteration 517, loss = 46625599.52394906\n",
            "Iteration 518, loss = 46490935.34033097\n",
            "Iteration 519, loss = 46359176.12953993\n",
            "Iteration 520, loss = 46216100.87406477\n",
            "Iteration 521, loss = 46047510.99870890\n",
            "Iteration 522, loss = 45894867.63332420\n",
            "Iteration 523, loss = 45731172.03615343\n",
            "Iteration 524, loss = 45592167.74776311\n",
            "Iteration 525, loss = 45431827.78361633\n",
            "Iteration 526, loss = 45284617.20055937\n",
            "Iteration 527, loss = 45142451.75380729\n",
            "Iteration 528, loss = 44991150.91283540\n",
            "Iteration 529, loss = 44821465.76133138\n",
            "Iteration 530, loss = 44653735.28197967\n",
            "Iteration 531, loss = 44494515.37009516\n",
            "Iteration 532, loss = 44333017.86962113\n",
            "Iteration 533, loss = 44180603.57028034\n",
            "Iteration 534, loss = 44030785.97480175\n",
            "Iteration 535, loss = 43851254.22640334\n",
            "Iteration 536, loss = 43699492.82878437\n",
            "Iteration 537, loss = 43590407.94546972\n",
            "Iteration 538, loss = 43382059.41196541\n",
            "Iteration 539, loss = 43226525.46647469\n",
            "Iteration 540, loss = 43036629.30437122\n",
            "Iteration 541, loss = 42858486.06940670\n",
            "Iteration 542, loss = 42720813.24389268\n",
            "Iteration 543, loss = 42530346.80646633\n",
            "Iteration 544, loss = 42383645.22785632\n",
            "Iteration 545, loss = 42198684.92695421\n",
            "Iteration 546, loss = 42020442.64021790\n",
            "Iteration 547, loss = 41859594.27345108\n",
            "Iteration 548, loss = 41694173.00892650\n",
            "Iteration 549, loss = 41501731.29752637\n",
            "Iteration 550, loss = 41323494.08595074\n",
            "Iteration 551, loss = 41219891.06370262\n",
            "Iteration 552, loss = 41001721.84411256\n",
            "Iteration 553, loss = 40819528.17317211\n",
            "Iteration 554, loss = 40642383.83691704\n",
            "Iteration 555, loss = 40452703.24738372\n",
            "Iteration 556, loss = 40264390.58257249\n",
            "Iteration 557, loss = 40084889.53673752\n",
            "Iteration 558, loss = 39911283.61944003\n",
            "Iteration 559, loss = 39738126.13302177\n",
            "Iteration 560, loss = 39553061.36216343\n",
            "Iteration 561, loss = 39347971.26435044\n",
            "Iteration 562, loss = 39220142.41015654\n",
            "Iteration 563, loss = 38998116.45649504\n",
            "Iteration 564, loss = 38799959.40400950\n",
            "Iteration 565, loss = 38638177.63359942\n",
            "Iteration 566, loss = 38435746.79718730\n",
            "Iteration 567, loss = 38253237.40086745\n",
            "Iteration 568, loss = 38085918.69075029\n",
            "Iteration 569, loss = 37871032.36342826\n",
            "Iteration 570, loss = 37699374.53929586\n",
            "Iteration 571, loss = 37498801.44823901\n",
            "Iteration 572, loss = 37342113.00826503\n",
            "Iteration 573, loss = 37159073.88971900\n",
            "Iteration 574, loss = 36957574.08010425\n",
            "Iteration 575, loss = 36766627.12530918\n",
            "Iteration 576, loss = 36569999.03626868\n",
            "Iteration 577, loss = 36395085.50893056\n",
            "Iteration 578, loss = 36181564.74466825\n",
            "Iteration 579, loss = 36022981.36506689\n",
            "Iteration 580, loss = 35819638.21234035\n",
            "Iteration 581, loss = 35640736.44955055\n",
            "Iteration 582, loss = 35429292.73937539\n",
            "Iteration 583, loss = 35260797.07524043\n",
            "Iteration 584, loss = 35076862.87439027\n",
            "Iteration 585, loss = 34873651.64187630\n",
            "Iteration 586, loss = 34721646.80240093\n",
            "Iteration 587, loss = 34583153.79007182\n",
            "Iteration 588, loss = 34338319.21424459\n",
            "Iteration 589, loss = 34110962.19012749\n",
            "Iteration 590, loss = 33969645.58475367\n",
            "Iteration 591, loss = 33765110.16994411\n",
            "Iteration 592, loss = 33612558.53405876\n",
            "Iteration 593, loss = 33419955.80548226\n",
            "Iteration 594, loss = 33193980.89547708\n",
            "Iteration 595, loss = 33008538.29375884\n",
            "Iteration 596, loss = 32833436.80707638\n",
            "Iteration 597, loss = 32634648.03117106\n",
            "Iteration 598, loss = 32438655.79837778\n",
            "Iteration 599, loss = 32257517.24166145\n",
            "Iteration 600, loss = 32091279.02171658\n",
            "Iteration 601, loss = 31890478.49698884\n",
            "Iteration 602, loss = 31743171.61474985\n",
            "Iteration 603, loss = 31545496.98541965\n",
            "Iteration 604, loss = 31390850.59015103\n",
            "Iteration 605, loss = 31212551.41855744\n",
            "Iteration 606, loss = 31002444.33965352\n",
            "Iteration 607, loss = 30799873.76642040\n",
            "Iteration 608, loss = 30664538.20586198\n",
            "Iteration 609, loss = 30473872.63796066\n",
            "Iteration 610, loss = 30266692.66368707\n",
            "Iteration 611, loss = 30129028.13599734\n",
            "Iteration 612, loss = 29983525.96781863\n",
            "Iteration 613, loss = 29762641.97993072\n",
            "Iteration 614, loss = 29608407.47470518\n",
            "Iteration 615, loss = 29409866.06956920\n",
            "Iteration 616, loss = 29245860.82287036\n",
            "Iteration 617, loss = 29142563.14384117\n",
            "Iteration 618, loss = 28924705.61292495\n",
            "Iteration 619, loss = 28810055.65731391\n",
            "Iteration 620, loss = 28593907.95711191\n",
            "Iteration 621, loss = 28451050.52245907\n",
            "Iteration 622, loss = 28261329.27553064\n",
            "Iteration 623, loss = 28101350.41126036\n",
            "Iteration 624, loss = 27952169.33572933\n",
            "Iteration 625, loss = 27802199.69965380\n",
            "Iteration 626, loss = 27643031.92388439\n",
            "Iteration 627, loss = 27514633.38936583\n",
            "Iteration 628, loss = 27314059.25622209\n",
            "Iteration 629, loss = 27226475.27265067\n",
            "Iteration 630, loss = 27027652.82995716\n",
            "Iteration 631, loss = 26911364.32354686\n",
            "Iteration 632, loss = 26812023.31722946\n",
            "Iteration 633, loss = 26618497.36738425\n",
            "Iteration 634, loss = 26486783.32246240\n",
            "Iteration 635, loss = 26343152.64411764\n",
            "Iteration 636, loss = 26149464.63176037\n",
            "Iteration 637, loss = 26019864.31695772\n",
            "Iteration 638, loss = 25883935.73244298\n",
            "Iteration 639, loss = 25747795.03666576\n",
            "Iteration 640, loss = 25629981.31460109\n",
            "Iteration 641, loss = 25475373.16248582\n",
            "Iteration 642, loss = 25348966.55577739\n",
            "Iteration 643, loss = 25232437.00835954\n",
            "Iteration 644, loss = 25122173.58155948\n",
            "Iteration 645, loss = 25026067.86956094\n",
            "Iteration 646, loss = 24858383.94649064\n",
            "Iteration 647, loss = 24705218.95623770\n",
            "Iteration 648, loss = 24664510.99304216\n",
            "Iteration 649, loss = 24461825.61752862\n",
            "Iteration 650, loss = 24406505.27556129\n",
            "Iteration 651, loss = 24249393.79009669\n",
            "Iteration 652, loss = 24146056.65165254\n",
            "Iteration 653, loss = 24046864.88836960\n",
            "Iteration 654, loss = 23925416.50684952\n",
            "Iteration 655, loss = 23781251.69841821\n",
            "Iteration 656, loss = 23701647.27050001\n",
            "Iteration 657, loss = 23577028.61444949\n",
            "Iteration 658, loss = 23479738.90494446\n",
            "Iteration 659, loss = 23377520.00145218\n",
            "Iteration 660, loss = 23272471.47335377\n",
            "Iteration 661, loss = 23173296.18444833\n",
            "Iteration 662, loss = 23078221.53114640\n",
            "Iteration 663, loss = 22988535.79414313\n",
            "Iteration 664, loss = 22920466.63912677\n",
            "Iteration 665, loss = 22786229.52981931\n",
            "Iteration 666, loss = 22718094.42277772\n",
            "Iteration 667, loss = 22610302.08498978\n",
            "Iteration 668, loss = 22536400.61434642\n",
            "Iteration 669, loss = 22462754.17220300\n",
            "Iteration 670, loss = 22374098.35793175\n",
            "Iteration 671, loss = 22266505.02276834\n",
            "Iteration 672, loss = 22212048.88625132\n",
            "Iteration 673, loss = 22106512.33852062\n",
            "Iteration 674, loss = 22046453.60720462\n",
            "Iteration 675, loss = 21957242.23243912\n",
            "Iteration 676, loss = 21884604.84233187\n",
            "Iteration 677, loss = 21817329.64435401\n",
            "Iteration 678, loss = 21812233.23312616\n",
            "Iteration 679, loss = 21685929.28704382\n",
            "Iteration 680, loss = 21608039.04709405\n",
            "Iteration 681, loss = 21531494.92946042\n",
            "Iteration 682, loss = 21464145.05671751\n",
            "Iteration 683, loss = 21424725.26204994\n",
            "Iteration 684, loss = 21335012.76135502\n",
            "Iteration 685, loss = 21281526.33134672\n",
            "Iteration 686, loss = 21233358.06825852\n",
            "Iteration 687, loss = 21153914.29497383\n",
            "Iteration 688, loss = 21095364.48657990\n",
            "Iteration 689, loss = 21038429.66258674\n",
            "Iteration 690, loss = 20997976.11082723\n",
            "Iteration 691, loss = 20927335.00403800\n",
            "Iteration 692, loss = 20889300.69091148\n",
            "Iteration 693, loss = 20881694.50071857\n",
            "Iteration 694, loss = 20766454.58325892\n",
            "Iteration 695, loss = 20768734.78376199\n",
            "Iteration 696, loss = 20687545.19278059\n",
            "Iteration 697, loss = 20643678.21320754\n",
            "Iteration 698, loss = 20623034.78170511\n",
            "Iteration 699, loss = 20597421.81465316\n",
            "Iteration 700, loss = 20485577.71848541\n",
            "Iteration 701, loss = 20520416.44312430\n",
            "Iteration 702, loss = 20401766.38797881\n",
            "Iteration 703, loss = 20421586.02211179\n",
            "Iteration 704, loss = 20378038.96992761\n",
            "Iteration 705, loss = 20296676.35624187\n",
            "Iteration 706, loss = 20312329.10077707\n",
            "Iteration 707, loss = 20246394.62347818\n",
            "Iteration 708, loss = 20224700.08291831\n",
            "Iteration 709, loss = 20192923.94581761\n",
            "Iteration 710, loss = 20122692.73313118\n",
            "Iteration 711, loss = 20138942.78085066\n",
            "Iteration 712, loss = 20037169.30009432\n",
            "Iteration 713, loss = 20048821.29092451\n",
            "Iteration 714, loss = 20000166.81199887\n",
            "Iteration 715, loss = 19940406.29749699\n",
            "Iteration 716, loss = 19949316.04596538\n",
            "Iteration 717, loss = 19920358.85293972\n",
            "Iteration 718, loss = 19912136.00032426\n",
            "Iteration 719, loss = 19831751.72103518\n",
            "Iteration 720, loss = 19832197.92766240\n",
            "Iteration 721, loss = 19772931.67123130\n",
            "Iteration 722, loss = 19814088.91037259\n",
            "Iteration 723, loss = 19734218.11598873\n",
            "Iteration 724, loss = 19770107.38279090\n",
            "Iteration 725, loss = 19683105.23101914\n",
            "Iteration 726, loss = 19713918.67588630\n",
            "Iteration 727, loss = 19699506.47101770\n",
            "Iteration 728, loss = 19617910.10329462\n",
            "Iteration 729, loss = 19630683.71996706\n",
            "Iteration 730, loss = 19598510.56900435\n",
            "Iteration 731, loss = 19660076.36658034\n",
            "Iteration 732, loss = 19547219.45704940\n",
            "Iteration 733, loss = 19553530.30787872\n",
            "Iteration 734, loss = 19519409.22118103\n",
            "Iteration 735, loss = 19531268.40139423\n",
            "Iteration 736, loss = 19486928.85938466\n",
            "Iteration 737, loss = 19502082.35306393\n",
            "Iteration 738, loss = 19456300.28553054\n",
            "Iteration 739, loss = 19451904.19144088\n",
            "Iteration 740, loss = 19407487.81627261\n",
            "Iteration 741, loss = 19395684.37248892\n",
            "Iteration 742, loss = 19400831.04488410\n",
            "Iteration 743, loss = 19364525.07241756\n",
            "Iteration 744, loss = 19351098.69570944\n",
            "Iteration 745, loss = 19336117.67974716\n",
            "Iteration 746, loss = 19329890.84710483\n",
            "Iteration 747, loss = 19314106.60393216\n",
            "Iteration 748, loss = 19293216.67105390\n",
            "Iteration 749, loss = 19305395.51771826\n",
            "Iteration 750, loss = 19279766.24667862\n",
            "Iteration 751, loss = 19344171.70484208\n",
            "Iteration 752, loss = 19304386.68821238\n",
            "Iteration 753, loss = 19266652.09168002\n",
            "Iteration 754, loss = 19269443.53157339\n",
            "Iteration 755, loss = 19222212.08924160\n",
            "Iteration 756, loss = 19232457.20665289\n",
            "Iteration 757, loss = 19223481.20634839\n",
            "Iteration 758, loss = 19201903.37391741\n",
            "Iteration 759, loss = 19193029.02083942\n",
            "Iteration 760, loss = 19204372.06313755\n",
            "Iteration 761, loss = 19172576.61487966\n",
            "Iteration 762, loss = 19153199.12422747\n",
            "Iteration 763, loss = 19168287.32689905\n",
            "Iteration 764, loss = 19141132.90979451\n",
            "Iteration 765, loss = 19185133.62497159\n",
            "Iteration 766, loss = 19109490.91700913\n",
            "Iteration 767, loss = 19140575.66284259\n",
            "Iteration 768, loss = 19120263.97590035\n",
            "Iteration 769, loss = 19146892.67545858\n",
            "Iteration 770, loss = 19116225.42396823\n",
            "Iteration 771, loss = 19189381.69647452\n",
            "Iteration 772, loss = 19138156.00853686\n",
            "Iteration 773, loss = 19079921.81994113\n",
            "Iteration 774, loss = 19059222.55164263\n",
            "Iteration 775, loss = 19103003.43478947\n",
            "Iteration 776, loss = 19034274.74817719\n",
            "Iteration 777, loss = 19051784.37017347\n",
            "Iteration 778, loss = 19024480.59079585\n",
            "Iteration 779, loss = 19029782.71697800\n",
            "Iteration 780, loss = 19000277.87737867\n",
            "Iteration 781, loss = 19079291.35776046\n",
            "Iteration 782, loss = 19008179.18330376\n",
            "Iteration 783, loss = 19048912.43187831\n",
            "Iteration 784, loss = 18981546.51377069\n",
            "Iteration 785, loss = 19034201.22173727\n",
            "Iteration 786, loss = 18983610.83788707\n",
            "Iteration 787, loss = 19006201.26128010\n",
            "Iteration 788, loss = 18957695.62021630\n",
            "Iteration 789, loss = 18968811.27315186\n",
            "Iteration 790, loss = 18972019.01880736\n",
            "Iteration 791, loss = 18970221.56746440\n",
            "Iteration 792, loss = 18937265.35708741\n",
            "Iteration 793, loss = 18962489.72507762\n",
            "Iteration 794, loss = 18935826.73850086\n",
            "Iteration 795, loss = 18924965.48945495\n",
            "Iteration 796, loss = 18935869.99404762\n",
            "Iteration 797, loss = 18963654.64099842\n",
            "Iteration 798, loss = 18942748.00091191\n",
            "Iteration 799, loss = 18928988.70884648\n",
            "Iteration 800, loss = 18913094.32973889\n",
            "Iteration 801, loss = 18913704.23514888\n",
            "Iteration 802, loss = 18900614.20719556\n",
            "Iteration 803, loss = 18916225.37695347\n",
            "Iteration 804, loss = 18916339.39354910\n",
            "Iteration 805, loss = 18879180.55100532\n",
            "Iteration 806, loss = 18901354.47548589\n",
            "Iteration 807, loss = 18864159.32044181\n",
            "Iteration 808, loss = 18876476.65756217\n",
            "Iteration 809, loss = 18909687.20164574\n",
            "Iteration 810, loss = 18858605.16924436\n",
            "Iteration 811, loss = 18857883.38583467\n",
            "Iteration 812, loss = 18858864.68681046\n",
            "Iteration 813, loss = 18840647.20551504\n",
            "Iteration 814, loss = 18846388.24784473\n",
            "Iteration 815, loss = 18810918.11042803\n",
            "Iteration 816, loss = 18822859.13597125\n",
            "Iteration 817, loss = 18829339.82307369\n",
            "Iteration 818, loss = 18806292.77313962\n",
            "Iteration 819, loss = 18822192.56694626\n",
            "Iteration 820, loss = 18849431.72143549\n",
            "Iteration 821, loss = 18788106.58343592\n",
            "Iteration 822, loss = 18821188.63198981\n",
            "Iteration 823, loss = 18766250.91488034\n",
            "Iteration 824, loss = 18820664.05967588\n",
            "Iteration 825, loss = 18799169.19928545\n",
            "Iteration 826, loss = 18773244.70708077\n",
            "Iteration 827, loss = 18774969.13026001\n",
            "Iteration 828, loss = 18793073.85610722\n",
            "Iteration 829, loss = 18766253.87422283\n",
            "Iteration 830, loss = 18754224.43326740\n",
            "Iteration 831, loss = 18752104.44774213\n",
            "Iteration 832, loss = 18753485.07146060\n",
            "Iteration 833, loss = 18748676.13349384\n",
            "Iteration 834, loss = 18734669.18572982\n",
            "Iteration 835, loss = 18805286.60139918\n",
            "Iteration 836, loss = 18713157.59016283\n",
            "Iteration 837, loss = 18773409.93614250\n",
            "Iteration 838, loss = 18839236.75642473\n",
            "Iteration 839, loss = 18760847.09779096\n",
            "Iteration 840, loss = 18726352.73981514\n",
            "Iteration 841, loss = 18692910.98933008\n",
            "Iteration 842, loss = 18701597.07981677\n",
            "Iteration 843, loss = 18713594.37050155\n",
            "Iteration 844, loss = 18688997.30160918\n",
            "Iteration 845, loss = 18700953.02578713\n",
            "Iteration 846, loss = 18696925.12998372\n",
            "Iteration 847, loss = 18686829.52643778\n",
            "Iteration 848, loss = 18688193.95565351\n",
            "Iteration 849, loss = 18680755.42419417\n",
            "Iteration 850, loss = 18675143.60898386\n",
            "Iteration 851, loss = 18666175.23405631\n",
            "Iteration 852, loss = 18691570.98729422\n",
            "Iteration 853, loss = 18663660.77409008\n",
            "Iteration 854, loss = 18647925.25448281\n",
            "Iteration 855, loss = 18653230.12704764\n",
            "Iteration 856, loss = 18646939.57994364\n",
            "Iteration 857, loss = 18642614.00291267\n",
            "Iteration 858, loss = 18689398.34555155\n",
            "Iteration 859, loss = 18637536.98252948\n",
            "Iteration 860, loss = 18646125.27289941\n",
            "Iteration 861, loss = 18633467.24058963\n",
            "Iteration 862, loss = 18632706.80159460\n",
            "Iteration 863, loss = 18615563.05033530\n",
            "Iteration 864, loss = 18619852.55000422\n",
            "Iteration 865, loss = 18614580.60095719\n",
            "Iteration 866, loss = 18607541.35388872\n",
            "Iteration 867, loss = 18613434.12624878\n",
            "Iteration 868, loss = 18605182.77486197\n",
            "Iteration 869, loss = 18605634.65723201\n",
            "Iteration 870, loss = 18605592.01104341\n",
            "Iteration 871, loss = 18600330.96475479\n",
            "Iteration 872, loss = 18601503.62923720\n",
            "Iteration 873, loss = 18583346.32314917\n",
            "Iteration 874, loss = 18622741.95569224\n",
            "Iteration 875, loss = 18603240.42230988\n",
            "Iteration 876, loss = 18640763.44638342\n",
            "Iteration 877, loss = 18675740.41318667\n",
            "Iteration 878, loss = 18587208.23330436\n",
            "Iteration 879, loss = 18587835.41234922\n",
            "Iteration 880, loss = 18579118.58150887\n",
            "Iteration 881, loss = 18562531.35840081\n",
            "Iteration 882, loss = 18579107.39234035\n",
            "Iteration 883, loss = 18571140.75662040\n",
            "Iteration 884, loss = 18559260.34900934\n",
            "Iteration 885, loss = 18553030.91065282\n",
            "Iteration 886, loss = 18544843.70487097\n",
            "Iteration 887, loss = 18640975.68778091\n",
            "Iteration 888, loss = 18528038.04282355\n",
            "Iteration 889, loss = 18577177.80601328\n",
            "Iteration 890, loss = 18542875.02744828\n",
            "Iteration 891, loss = 18610314.45214368\n",
            "Iteration 892, loss = 18516094.05958777\n",
            "Iteration 893, loss = 18549768.68833552\n",
            "Iteration 894, loss = 18549891.28527562\n",
            "Iteration 895, loss = 18538321.29124955\n",
            "Iteration 896, loss = 18524097.37058184\n",
            "Iteration 897, loss = 18514617.49383210\n",
            "Iteration 898, loss = 18508258.40587575\n",
            "Iteration 899, loss = 18516205.85344436\n",
            "Iteration 900, loss = 18501724.79395758\n",
            "Iteration 901, loss = 18499652.04587124\n",
            "Iteration 902, loss = 18519905.09945849\n",
            "Iteration 903, loss = 18499154.27429805\n",
            "Iteration 904, loss = 18510162.91691511\n",
            "Iteration 905, loss = 18496555.23867640\n",
            "Iteration 906, loss = 18492264.23871217\n",
            "Iteration 907, loss = 18483662.90401476\n",
            "Iteration 908, loss = 18482699.21512412\n",
            "Iteration 909, loss = 18496513.04211778\n",
            "Iteration 910, loss = 18486259.77869587\n",
            "Iteration 911, loss = 18478407.65208489\n",
            "Iteration 912, loss = 18467648.77200205\n",
            "Iteration 913, loss = 18480614.67635046\n",
            "Iteration 914, loss = 18483618.07545217\n",
            "Iteration 915, loss = 18457098.99566771\n",
            "Iteration 916, loss = 18464285.94483965\n",
            "Iteration 917, loss = 18459283.24446309\n",
            "Iteration 918, loss = 18452355.90048338\n",
            "Iteration 919, loss = 18475143.83439051\n",
            "Iteration 920, loss = 18445320.02478499\n",
            "Iteration 921, loss = 18460127.27712737\n",
            "Iteration 922, loss = 18451142.12717737\n",
            "Iteration 923, loss = 18468489.34083955\n",
            "Iteration 924, loss = 18452242.96711572\n",
            "Iteration 925, loss = 18432102.55606597\n",
            "Iteration 926, loss = 18425337.81933096\n",
            "Iteration 927, loss = 18425616.50828369\n",
            "Iteration 928, loss = 18476658.42620211\n",
            "Iteration 929, loss = 18428827.18270055\n",
            "Iteration 930, loss = 18417618.14465362\n",
            "Iteration 931, loss = 18426566.33032480\n",
            "Iteration 932, loss = 18408932.95350814\n",
            "Iteration 933, loss = 18442822.59081840\n",
            "Iteration 934, loss = 18402692.74741326\n",
            "Iteration 935, loss = 18511481.60033408\n",
            "Iteration 936, loss = 18508963.90463518\n",
            "Iteration 937, loss = 18414552.20395504\n",
            "Iteration 938, loss = 18419498.43313498\n",
            "Iteration 939, loss = 18441203.90215835\n",
            "Iteration 940, loss = 18398908.32903181\n",
            "Iteration 941, loss = 18389337.85875390\n",
            "Iteration 942, loss = 18462107.16928643\n",
            "Iteration 943, loss = 18369270.28182829\n",
            "Iteration 944, loss = 18487393.25336225\n",
            "Iteration 945, loss = 18396450.97972102\n",
            "Iteration 946, loss = 18425279.18627027\n",
            "Iteration 947, loss = 18370191.45359740\n",
            "Iteration 948, loss = 18379953.68542527\n",
            "Iteration 949, loss = 18384813.97202874\n",
            "Iteration 950, loss = 18357436.01905169\n",
            "Iteration 951, loss = 18373500.90146845\n",
            "Iteration 952, loss = 18353859.25590810\n",
            "Iteration 953, loss = 18380767.88769269\n",
            "Iteration 954, loss = 18353682.58278554\n",
            "Iteration 955, loss = 18384141.30289892\n",
            "Iteration 956, loss = 18365108.99150451\n",
            "Iteration 957, loss = 18371109.33650307\n",
            "Iteration 958, loss = 18344597.63024185\n",
            "Iteration 959, loss = 18341985.17174272\n",
            "Iteration 960, loss = 18349206.37703958\n",
            "Iteration 961, loss = 18348321.93279166\n",
            "Iteration 962, loss = 18345208.44839856\n",
            "Iteration 963, loss = 18329088.05529412\n",
            "Iteration 964, loss = 18329844.20331967\n",
            "Iteration 965, loss = 18323172.93331158\n",
            "Iteration 966, loss = 18322329.40419599\n",
            "Iteration 967, loss = 18319955.71729271\n",
            "Iteration 968, loss = 18320967.80227728\n",
            "Iteration 969, loss = 18322047.51136306\n",
            "Iteration 970, loss = 18353952.41572060\n",
            "Iteration 971, loss = 18334973.53265459\n",
            "Iteration 972, loss = 18321563.92180891\n",
            "Iteration 973, loss = 18324998.48965431\n",
            "Iteration 974, loss = 18294526.32259361\n",
            "Iteration 975, loss = 18304722.24538221\n",
            "Iteration 976, loss = 18291678.10289067\n",
            "Iteration 977, loss = 18285433.73420046\n",
            "Iteration 978, loss = 18278479.06686665\n",
            "Iteration 979, loss = 18279496.76937338\n",
            "Iteration 980, loss = 18275874.42727948\n",
            "Iteration 981, loss = 18271401.12075678\n",
            "Iteration 982, loss = 18268003.23962020\n",
            "Iteration 983, loss = 18267870.58083135\n",
            "Iteration 984, loss = 18282165.48002953\n",
            "Iteration 985, loss = 18259530.28824472\n",
            "Iteration 986, loss = 18304232.81825061\n",
            "Iteration 987, loss = 18275880.09013063\n",
            "Iteration 988, loss = 18256057.42785014\n",
            "Iteration 989, loss = 18278577.05495798\n",
            "Iteration 990, loss = 18239746.07948584\n",
            "Iteration 991, loss = 18245612.08752484\n",
            "Iteration 992, loss = 18244185.24561622\n",
            "Iteration 993, loss = 18238060.63576924\n",
            "Iteration 994, loss = 18231592.69595377\n",
            "Iteration 995, loss = 18227006.30972541\n",
            "Iteration 996, loss = 18234468.09598242\n",
            "Iteration 997, loss = 18232722.53287431\n",
            "Iteration 998, loss = 18237205.74184455\n",
            "Iteration 999, loss = 18215881.18689525\n",
            "Iteration 1000, loss = 18232325.76447520\n",
            "Iteration 1001, loss = 18205083.60032343\n",
            "Iteration 1002, loss = 18219628.62584497\n",
            "Iteration 1003, loss = 18235078.17549334\n",
            "Iteration 1004, loss = 18223112.95005808\n",
            "Iteration 1005, loss = 18208242.15454002\n",
            "Iteration 1006, loss = 18200463.31357387\n",
            "Iteration 1007, loss = 18204503.56822120\n",
            "Iteration 1008, loss = 18198664.13570173\n",
            "Iteration 1009, loss = 18242203.87970295\n",
            "Iteration 1010, loss = 18219951.05477298\n",
            "Iteration 1011, loss = 18197581.24606344\n",
            "Iteration 1012, loss = 18211137.15414638\n",
            "Iteration 1013, loss = 18201637.62933639\n",
            "Iteration 1014, loss = 18180252.79259508\n",
            "Iteration 1015, loss = 18184889.49749383\n",
            "Iteration 1016, loss = 18181999.23662276\n",
            "Iteration 1017, loss = 18178402.10862447\n",
            "Iteration 1018, loss = 18211673.69698612\n",
            "Iteration 1019, loss = 18170117.88891724\n",
            "Iteration 1020, loss = 18167799.77369574\n",
            "Iteration 1021, loss = 18174553.53718513\n",
            "Iteration 1022, loss = 18162378.99882432\n",
            "Iteration 1023, loss = 18158921.50906486\n",
            "Iteration 1024, loss = 18179053.99341128\n",
            "Iteration 1025, loss = 18161499.67546704\n",
            "Iteration 1026, loss = 18152523.44002521\n",
            "Iteration 1027, loss = 18161205.55210719\n",
            "Iteration 1028, loss = 18192816.88231592\n",
            "Iteration 1029, loss = 18158402.21433167\n",
            "Iteration 1030, loss = 18134757.24917786\n",
            "Iteration 1031, loss = 18150380.00130764\n",
            "Iteration 1032, loss = 18136814.80265131\n",
            "Iteration 1033, loss = 18142293.17908198\n",
            "Iteration 1034, loss = 18128807.52901202\n",
            "Iteration 1035, loss = 18171765.49356577\n",
            "Iteration 1036, loss = 18122701.90009141\n",
            "Iteration 1037, loss = 18121754.69342796\n",
            "Iteration 1038, loss = 18126374.82129505\n",
            "Iteration 1039, loss = 18132420.50108621\n",
            "Iteration 1040, loss = 18137099.89087823\n",
            "Iteration 1041, loss = 18125941.37227612\n",
            "Iteration 1042, loss = 18119104.60418085\n",
            "Iteration 1043, loss = 18119256.43890672\n",
            "Iteration 1044, loss = 18108195.41918852\n",
            "Iteration 1045, loss = 18113136.49229765\n",
            "Iteration 1046, loss = 18136114.22675977\n",
            "Iteration 1047, loss = 18113531.75415462\n",
            "Iteration 1048, loss = 18120531.53713862\n",
            "Iteration 1049, loss = 18124482.60455327\n",
            "Iteration 1050, loss = 18117074.34454110\n",
            "Iteration 1051, loss = 18083267.67872986\n",
            "Iteration 1052, loss = 18084916.29295276\n",
            "Iteration 1053, loss = 18081014.48854647\n",
            "Iteration 1054, loss = 18090471.29623690\n",
            "Iteration 1055, loss = 18118073.93937232\n",
            "Iteration 1056, loss = 18118009.26263352\n",
            "Iteration 1057, loss = 18104976.15470941\n",
            "Iteration 1058, loss = 18098510.71055075\n",
            "Iteration 1059, loss = 18078977.64566334\n",
            "Iteration 1060, loss = 18092669.02321981\n",
            "Iteration 1061, loss = 18118113.61740699\n",
            "Iteration 1062, loss = 18097113.30264957\n",
            "Iteration 1063, loss = 18088699.87909080\n",
            "Iteration 1064, loss = 18057637.84521365\n",
            "Iteration 1065, loss = 18091428.74298507\n",
            "Iteration 1066, loss = 18036990.22603505\n",
            "Iteration 1067, loss = 18089057.88539288\n",
            "Iteration 1068, loss = 18044336.13322140\n",
            "Iteration 1069, loss = 18075364.65232408\n",
            "Iteration 1070, loss = 18082391.10600083\n",
            "Iteration 1071, loss = 18051572.14795896\n",
            "Iteration 1072, loss = 18062803.51760422\n",
            "Iteration 1073, loss = 18059897.64947652\n",
            "Iteration 1074, loss = 18039526.34255395\n",
            "Iteration 1075, loss = 18048609.58411398\n",
            "Iteration 1076, loss = 18034756.61534346\n",
            "Iteration 1077, loss = 18038938.81035827\n",
            "Iteration 1078, loss = 18022772.24159796\n",
            "Iteration 1079, loss = 18008407.59994328\n",
            "Iteration 1080, loss = 18038320.48276207\n",
            "Iteration 1081, loss = 18015189.74635741\n",
            "Iteration 1082, loss = 18013870.10126602\n",
            "Iteration 1083, loss = 18012425.15059392\n",
            "Iteration 1084, loss = 18005255.51497326\n",
            "Iteration 1085, loss = 18029440.10623481\n",
            "Iteration 1086, loss = 18048724.66481959\n",
            "Iteration 1087, loss = 18001286.09316046\n",
            "Iteration 1088, loss = 17997029.07420081\n",
            "Iteration 1089, loss = 18006852.60185384\n",
            "Iteration 1090, loss = 18014795.74602005\n",
            "Iteration 1091, loss = 17983078.11535117\n",
            "Iteration 1092, loss = 17999425.10257265\n",
            "Iteration 1093, loss = 17985597.82011488\n",
            "Iteration 1094, loss = 17984838.84974124\n",
            "Iteration 1095, loss = 17984323.44946852\n",
            "Iteration 1096, loss = 17979464.18370378\n",
            "Iteration 1097, loss = 17977075.58578002\n",
            "Iteration 1098, loss = 17983859.72623288\n",
            "Iteration 1099, loss = 17952724.45628959\n",
            "Iteration 1100, loss = 17981207.92326547\n",
            "Iteration 1101, loss = 17960324.94658168\n",
            "Iteration 1102, loss = 18024238.29568489\n",
            "Iteration 1103, loss = 18011397.15182644\n",
            "Iteration 1104, loss = 17963711.73089978\n",
            "Iteration 1105, loss = 17952155.98577247\n",
            "Iteration 1106, loss = 17959952.89129610\n",
            "Iteration 1107, loss = 17959376.01701957\n",
            "Iteration 1108, loss = 17947870.49965552\n",
            "Iteration 1109, loss = 17944037.18039003\n",
            "Iteration 1110, loss = 17946849.39943061\n",
            "Iteration 1111, loss = 17931368.85190345\n",
            "Iteration 1112, loss = 17934968.29586576\n",
            "Iteration 1113, loss = 17928030.43087872\n",
            "Iteration 1114, loss = 17945763.27738585\n",
            "Iteration 1115, loss = 17925856.24251503\n",
            "Iteration 1116, loss = 17918537.95753307\n",
            "Iteration 1117, loss = 17931011.04529063\n",
            "Iteration 1118, loss = 17937489.64205688\n",
            "Iteration 1119, loss = 17926704.35386641\n",
            "Iteration 1120, loss = 17937601.26334176\n",
            "Iteration 1121, loss = 17919262.16303699\n",
            "Iteration 1122, loss = 17926359.92140139\n",
            "Iteration 1123, loss = 17914617.61670326\n",
            "Iteration 1124, loss = 17928757.49445061\n",
            "Iteration 1125, loss = 17909577.47738886\n",
            "Iteration 1126, loss = 17943132.82770045\n",
            "Iteration 1127, loss = 17920025.69597701\n",
            "Iteration 1128, loss = 17894555.00434164\n",
            "Iteration 1129, loss = 17905668.05029533\n",
            "Iteration 1130, loss = 17881138.94480461\n",
            "Iteration 1131, loss = 17893225.70324872\n",
            "Iteration 1132, loss = 17891226.28448351\n",
            "Iteration 1133, loss = 17885336.05605014\n",
            "Iteration 1134, loss = 17900578.13831673\n",
            "Iteration 1135, loss = 17879492.02643436\n",
            "Iteration 1136, loss = 17876361.52780572\n",
            "Iteration 1137, loss = 17894410.99141227\n",
            "Iteration 1138, loss = 17876786.80729776\n",
            "Iteration 1139, loss = 17870822.97887780\n",
            "Iteration 1140, loss = 17864457.40077595\n",
            "Iteration 1141, loss = 17893525.42965614\n",
            "Iteration 1142, loss = 17900000.91084663\n",
            "Iteration 1143, loss = 17889182.24462855\n",
            "Iteration 1144, loss = 17861500.82440767\n",
            "Iteration 1145, loss = 17869374.57858584\n",
            "Iteration 1146, loss = 17864666.33835532\n",
            "Iteration 1147, loss = 17845798.96654176\n",
            "Iteration 1148, loss = 17847515.97044681\n",
            "Iteration 1149, loss = 17840579.02061342\n",
            "Iteration 1150, loss = 17841150.61652353\n",
            "Iteration 1151, loss = 17841022.02059586\n",
            "Iteration 1152, loss = 17838789.75847998\n",
            "Iteration 1153, loss = 17869243.10701612\n",
            "Iteration 1154, loss = 17849594.14308701\n",
            "Iteration 1155, loss = 17836506.23897982\n",
            "Iteration 1156, loss = 17837099.50330690\n",
            "Iteration 1157, loss = 17844935.17578034\n",
            "Iteration 1158, loss = 17822649.74715259\n",
            "Iteration 1159, loss = 17861836.38981844\n",
            "Iteration 1160, loss = 17837575.81049003\n",
            "Iteration 1161, loss = 17819461.40955101\n",
            "Iteration 1162, loss = 17859687.60191623\n",
            "Iteration 1163, loss = 17847846.57963901\n",
            "Iteration 1164, loss = 17839721.75324284\n",
            "Iteration 1165, loss = 17800876.24187421\n",
            "Iteration 1166, loss = 17813750.37984041\n",
            "Iteration 1167, loss = 17817433.81858285\n",
            "Iteration 1168, loss = 17798794.29893426\n",
            "Iteration 1169, loss = 17812807.56128572\n",
            "Iteration 1170, loss = 17806991.93552150\n",
            "Iteration 1171, loss = 17777371.58837745\n",
            "Iteration 1172, loss = 17798718.39524996\n",
            "Iteration 1173, loss = 17780674.12635406\n",
            "Iteration 1174, loss = 17774779.72823146\n",
            "Iteration 1175, loss = 17793457.86231503\n",
            "Iteration 1176, loss = 17774056.00844727\n",
            "Iteration 1177, loss = 17780510.78156578\n",
            "Iteration 1178, loss = 17773909.67641510\n",
            "Iteration 1179, loss = 17779654.61320714\n",
            "Iteration 1180, loss = 17777673.47589256\n",
            "Iteration 1181, loss = 17765391.99515162\n",
            "Iteration 1182, loss = 17784997.99340326\n",
            "Iteration 1183, loss = 17752342.88965445\n",
            "Iteration 1184, loss = 17793566.15745063\n",
            "Iteration 1185, loss = 17767459.08525457\n",
            "Iteration 1186, loss = 17745610.33649674\n",
            "Iteration 1187, loss = 17752098.91668844\n",
            "Iteration 1188, loss = 17746579.30694994\n",
            "Iteration 1189, loss = 17772839.54288296\n",
            "Iteration 1190, loss = 17748083.35953093\n",
            "Iteration 1191, loss = 17759332.65104562\n",
            "Iteration 1192, loss = 17740514.71473793\n",
            "Iteration 1193, loss = 17739418.16525573\n",
            "Iteration 1194, loss = 17743914.92757126\n",
            "Iteration 1195, loss = 17752361.94794371\n",
            "Iteration 1196, loss = 17738029.84066428\n",
            "Iteration 1197, loss = 17740842.61862049\n",
            "Iteration 1198, loss = 17779484.45734591\n",
            "Iteration 1199, loss = 17712570.47640709\n",
            "Iteration 1200, loss = 17729440.33934627\n",
            "Iteration 1201, loss = 17716420.10468844\n",
            "Iteration 1202, loss = 17719047.10750037\n",
            "Iteration 1203, loss = 17744268.48117031\n",
            "Iteration 1204, loss = 17708229.04154718\n",
            "Iteration 1205, loss = 17704283.94593446\n",
            "Iteration 1206, loss = 17699332.96035875\n",
            "Iteration 1207, loss = 17702412.67560099\n",
            "Iteration 1208, loss = 17699856.64903281\n",
            "Iteration 1209, loss = 17706834.78837476\n",
            "Iteration 1210, loss = 17694198.08384030\n",
            "Iteration 1211, loss = 17693045.37455878\n",
            "Iteration 1212, loss = 17713842.79479410\n",
            "Iteration 1213, loss = 17680827.42635954\n",
            "Iteration 1214, loss = 17715982.36795695\n",
            "Iteration 1215, loss = 17736932.17846134\n",
            "Iteration 1216, loss = 17681613.09943829\n",
            "Iteration 1217, loss = 17663642.16853935\n",
            "Iteration 1218, loss = 17725740.52108328\n",
            "Iteration 1219, loss = 17694633.73858612\n",
            "Iteration 1220, loss = 17699066.19625734\n",
            "Iteration 1221, loss = 17697558.84402355\n",
            "Iteration 1222, loss = 17702675.13240425\n",
            "Iteration 1223, loss = 17708764.57624339\n",
            "Iteration 1224, loss = 17811735.80998719\n",
            "Iteration 1225, loss = 17668152.32343565\n",
            "Iteration 1226, loss = 17679032.64349719\n",
            "Iteration 1227, loss = 17689040.34284142\n",
            "Iteration 1228, loss = 17648438.05632989\n",
            "Iteration 1229, loss = 17661613.20176820\n",
            "Iteration 1230, loss = 17645468.43871857\n",
            "Iteration 1231, loss = 17649238.86178617\n",
            "Iteration 1232, loss = 17643837.67652675\n",
            "Iteration 1233, loss = 17640356.82768990\n",
            "Iteration 1234, loss = 17659847.71926373\n",
            "Iteration 1235, loss = 17655167.36089400\n",
            "Iteration 1236, loss = 17657338.19512248\n",
            "Iteration 1237, loss = 17733082.60603448\n",
            "Iteration 1238, loss = 17640124.40098720\n",
            "Iteration 1239, loss = 17663389.53903788\n",
            "Iteration 1240, loss = 17602995.76655699\n",
            "Iteration 1241, loss = 17641685.20428457\n",
            "Iteration 1242, loss = 17642810.19644889\n",
            "Iteration 1243, loss = 17619719.16234706\n",
            "Iteration 1244, loss = 17653367.94510055\n",
            "Iteration 1245, loss = 17600456.48674816\n",
            "Iteration 1246, loss = 17630526.69400594\n",
            "Iteration 1247, loss = 17618263.24569247\n",
            "Iteration 1248, loss = 17627777.56357102\n",
            "Iteration 1249, loss = 17605082.93963338\n",
            "Iteration 1250, loss = 17597652.82055224\n",
            "Iteration 1251, loss = 17594151.83105331\n",
            "Iteration 1252, loss = 17601498.50948397\n",
            "Iteration 1253, loss = 17587637.65685651\n",
            "Iteration 1254, loss = 17591924.33171668\n",
            "Iteration 1255, loss = 17597201.76706712\n",
            "Iteration 1256, loss = 17589764.24029221\n",
            "Iteration 1257, loss = 17579065.82345564\n",
            "Iteration 1258, loss = 17584768.70964650\n",
            "Iteration 1259, loss = 17570845.41797099\n",
            "Iteration 1260, loss = 17612297.21630591\n",
            "Iteration 1261, loss = 17576299.24775399\n",
            "Iteration 1262, loss = 17575519.58815827\n",
            "Iteration 1263, loss = 17586151.56995027\n",
            "Iteration 1264, loss = 17567028.19862401\n",
            "Iteration 1265, loss = 17560669.72841957\n",
            "Iteration 1266, loss = 17607834.53289919\n",
            "Iteration 1267, loss = 17608872.39884483\n",
            "Iteration 1268, loss = 17606871.65284859\n",
            "Iteration 1269, loss = 17618521.68940968\n",
            "Iteration 1270, loss = 17620793.10692237\n",
            "Iteration 1271, loss = 17552637.94031672\n",
            "Iteration 1272, loss = 17609463.09308191\n",
            "Iteration 1273, loss = 17526501.68914937\n",
            "Iteration 1274, loss = 17564796.22000194\n",
            "Iteration 1275, loss = 17540067.55536312\n",
            "Iteration 1276, loss = 17559047.26305760\n",
            "Iteration 1277, loss = 17550470.63049022\n",
            "Iteration 1278, loss = 17544310.92790530\n",
            "Iteration 1279, loss = 17568781.29939030\n",
            "Iteration 1280, loss = 17539456.43170563\n",
            "Iteration 1281, loss = 17562930.27536237\n",
            "Iteration 1282, loss = 17515739.82357832\n",
            "Iteration 1283, loss = 17526765.91564260\n",
            "Iteration 1284, loss = 17518339.83829680\n",
            "Iteration 1285, loss = 17524794.10470240\n",
            "Iteration 1286, loss = 17515422.93726836\n",
            "Iteration 1287, loss = 17540521.50610716\n",
            "Iteration 1288, loss = 17514304.09642634\n",
            "Iteration 1289, loss = 17560537.46921893\n",
            "Iteration 1290, loss = 17514401.56605594\n",
            "Iteration 1291, loss = 17541009.29102995\n",
            "Iteration 1292, loss = 17490953.08534841\n",
            "Iteration 1293, loss = 17527260.31624553\n",
            "Iteration 1294, loss = 17525735.52296956\n",
            "Iteration 1295, loss = 17501167.85100040\n",
            "Iteration 1296, loss = 17491778.27399372\n",
            "Iteration 1297, loss = 17487886.47629152\n",
            "Iteration 1298, loss = 17481822.73452547\n",
            "Iteration 1299, loss = 17557047.95732527\n",
            "Iteration 1300, loss = 17497677.01320485\n",
            "Iteration 1301, loss = 17515732.70704240\n",
            "Iteration 1302, loss = 17493714.68378880\n",
            "Iteration 1303, loss = 17503360.98442042\n",
            "Iteration 1304, loss = 17483737.94780140\n",
            "Iteration 1305, loss = 17491669.21649479\n",
            "Iteration 1306, loss = 17491992.10995153\n",
            "Iteration 1307, loss = 17455529.51369546\n",
            "Iteration 1308, loss = 17465540.80932499\n",
            "Iteration 1309, loss = 17469980.22690499\n",
            "Iteration 1310, loss = 17472582.51510311\n",
            "Iteration 1311, loss = 17454788.30638108\n",
            "Iteration 1312, loss = 17485146.28178084\n",
            "Iteration 1313, loss = 17472054.10892905\n",
            "Iteration 1314, loss = 17475810.76700691\n",
            "Iteration 1315, loss = 17456712.68560165\n",
            "Iteration 1316, loss = 17492068.12668676\n",
            "Iteration 1317, loss = 17488304.54083920\n",
            "Iteration 1318, loss = 17482124.50746624\n",
            "Iteration 1319, loss = 17442723.96414895\n",
            "Iteration 1320, loss = 17435260.60950562\n",
            "Iteration 1321, loss = 17432079.87117873\n",
            "Iteration 1322, loss = 17424807.88666349\n",
            "Iteration 1323, loss = 17416836.79065073\n",
            "Iteration 1324, loss = 17431315.16054914\n",
            "Iteration 1325, loss = 17426779.67836312\n",
            "Iteration 1326, loss = 17421433.44414188\n",
            "Iteration 1327, loss = 17428214.44304981\n",
            "Iteration 1328, loss = 17415464.81990568\n",
            "Iteration 1329, loss = 17408728.09467153\n",
            "Iteration 1330, loss = 17414477.93242256\n",
            "Iteration 1331, loss = 17402728.49835279\n",
            "Iteration 1332, loss = 17414835.31513141\n",
            "Iteration 1333, loss = 17419613.72901234\n",
            "Iteration 1334, loss = 17424841.25705427\n",
            "Iteration 1335, loss = 17400777.94409718\n",
            "Iteration 1336, loss = 17400704.43267410\n",
            "Iteration 1337, loss = 17400605.95064480\n",
            "Iteration 1338, loss = 17387204.04155919\n",
            "Iteration 1339, loss = 17431030.67024671\n",
            "Iteration 1340, loss = 17390482.39206006\n",
            "Iteration 1341, loss = 17387917.10343384\n",
            "Iteration 1342, loss = 17426999.78241954\n",
            "Iteration 1343, loss = 17399029.12167770\n",
            "Iteration 1344, loss = 17401395.06528325\n",
            "Iteration 1345, loss = 17384010.00598278\n",
            "Iteration 1346, loss = 17377484.06130770\n",
            "Iteration 1347, loss = 17464328.76417575\n",
            "Iteration 1348, loss = 17390702.45341909\n",
            "Iteration 1349, loss = 17372333.79206306\n",
            "Iteration 1350, loss = 17353458.54306425\n",
            "Iteration 1351, loss = 17361211.50025747\n",
            "Iteration 1352, loss = 17368408.02663108\n",
            "Iteration 1353, loss = 17371612.55701001\n",
            "Iteration 1354, loss = 17369787.43250542\n",
            "Iteration 1355, loss = 17365975.80802085\n",
            "Iteration 1356, loss = 17364059.34596235\n",
            "Iteration 1357, loss = 17357416.29573147\n",
            "Iteration 1358, loss = 17361501.74047523\n",
            "Iteration 1359, loss = 17340671.71855944\n",
            "Iteration 1360, loss = 17349415.55536145\n",
            "Iteration 1361, loss = 17332879.12308733\n",
            "Iteration 1362, loss = 17334768.13721951\n",
            "Iteration 1363, loss = 17326975.15688736\n",
            "Iteration 1364, loss = 17330885.23403633\n",
            "Iteration 1365, loss = 17327586.88932691\n",
            "Iteration 1366, loss = 17377378.23039550\n",
            "Iteration 1367, loss = 17338928.91966050\n",
            "Iteration 1368, loss = 17316482.29851595\n",
            "Iteration 1369, loss = 17339197.60547865\n",
            "Iteration 1370, loss = 17310862.82644719\n",
            "Iteration 1371, loss = 17310213.42429734\n",
            "Iteration 1372, loss = 17306099.90167694\n",
            "Iteration 1373, loss = 17321705.52435694\n",
            "Iteration 1374, loss = 17308846.64391837\n",
            "Iteration 1375, loss = 17297361.56505595\n",
            "Iteration 1376, loss = 17295276.72185608\n",
            "Iteration 1377, loss = 17287530.79920469\n",
            "Iteration 1378, loss = 17343076.67820690\n",
            "Iteration 1379, loss = 17283142.85948298\n",
            "Iteration 1380, loss = 17328477.31280673\n",
            "Iteration 1381, loss = 17275278.90363766\n",
            "Iteration 1382, loss = 17321835.65257501\n",
            "Iteration 1383, loss = 17302866.29855259\n",
            "Iteration 1384, loss = 17301363.00546348\n",
            "Iteration 1385, loss = 17277627.66018928\n",
            "Iteration 1386, loss = 17278938.94264146\n",
            "Iteration 1387, loss = 17272426.78385188\n",
            "Iteration 1388, loss = 17277771.16829493\n",
            "Iteration 1389, loss = 17261913.91737663\n",
            "Iteration 1390, loss = 17272958.90001677\n",
            "Iteration 1391, loss = 17271845.02086101\n",
            "Iteration 1392, loss = 17254772.81521871\n",
            "Iteration 1393, loss = 17250002.51843959\n",
            "Iteration 1394, loss = 17259253.80841131\n",
            "Iteration 1395, loss = 17253604.01198224\n",
            "Iteration 1396, loss = 17257495.00970763\n",
            "Iteration 1397, loss = 17236433.46247774\n",
            "Iteration 1398, loss = 17284107.97268713\n",
            "Iteration 1399, loss = 17263969.83549420\n",
            "Iteration 1400, loss = 17245353.10669686\n",
            "Iteration 1401, loss = 17239078.03449715\n",
            "Iteration 1402, loss = 17236668.25839990\n",
            "Iteration 1403, loss = 17235267.72330496\n",
            "Iteration 1404, loss = 17249992.49640334\n",
            "Iteration 1405, loss = 17252459.52615823\n",
            "Iteration 1406, loss = 17242448.63475093\n",
            "Iteration 1407, loss = 17223462.78441417\n",
            "Iteration 1408, loss = 17213074.54388045\n",
            "Iteration 1409, loss = 17222150.24533832\n",
            "Iteration 1410, loss = 17212916.23625840\n",
            "Iteration 1411, loss = 17215327.38167853\n",
            "Iteration 1412, loss = 17188362.89800482\n",
            "Iteration 1413, loss = 17238091.16928275\n",
            "Iteration 1414, loss = 17186702.87966229\n",
            "Iteration 1415, loss = 17207899.50012711\n",
            "Iteration 1416, loss = 17202606.54669512\n",
            "Iteration 1417, loss = 17197674.59714630\n",
            "Iteration 1418, loss = 17203235.90389179\n",
            "Iteration 1419, loss = 17201273.91648825\n",
            "Iteration 1420, loss = 17196183.78476370\n",
            "Iteration 1421, loss = 17176191.53635132\n",
            "Iteration 1422, loss = 17175237.72594852\n",
            "Iteration 1423, loss = 17199307.29527575\n",
            "Iteration 1424, loss = 17197593.67434005\n",
            "Iteration 1425, loss = 17158616.38679569\n",
            "Iteration 1426, loss = 17184282.58853040\n",
            "Iteration 1427, loss = 17170659.22728468\n",
            "Iteration 1428, loss = 17221842.13772470\n",
            "Iteration 1429, loss = 17170026.60617903\n",
            "Iteration 1430, loss = 17178976.37521520\n",
            "Iteration 1431, loss = 17169393.03772150\n",
            "Iteration 1432, loss = 17166016.12661982\n",
            "Iteration 1433, loss = 17148308.62334249\n",
            "Iteration 1434, loss = 17148644.80118435\n",
            "Iteration 1435, loss = 17156336.49627129\n",
            "Iteration 1436, loss = 17138722.49637510\n",
            "Iteration 1437, loss = 17141400.09941605\n",
            "Iteration 1438, loss = 17137865.61224244\n",
            "Iteration 1439, loss = 17139137.11636967\n",
            "Iteration 1440, loss = 17136299.46749796\n",
            "Iteration 1441, loss = 17132581.36546841\n",
            "Iteration 1442, loss = 17143061.44338179\n",
            "Iteration 1443, loss = 17121844.14845002\n",
            "Iteration 1444, loss = 17142262.06278904\n",
            "Iteration 1445, loss = 17124201.33024066\n",
            "Iteration 1446, loss = 17144116.74681464\n",
            "Iteration 1447, loss = 17125166.77334949\n",
            "Iteration 1448, loss = 17123712.81727168\n",
            "Iteration 1449, loss = 17113842.34615237\n",
            "Iteration 1450, loss = 17116121.48372437\n",
            "Iteration 1451, loss = 17131441.71589039\n",
            "Iteration 1452, loss = 17107004.58717342\n",
            "Iteration 1453, loss = 17103713.03511167\n",
            "Iteration 1454, loss = 17106304.29867408\n",
            "Iteration 1455, loss = 17099161.33108342\n",
            "Iteration 1456, loss = 17088490.48783874\n",
            "Iteration 1457, loss = 17095856.96810135\n",
            "Iteration 1458, loss = 17089009.60134261\n",
            "Iteration 1459, loss = 17093288.98105812\n",
            "Iteration 1460, loss = 17100175.22892695\n",
            "Iteration 1461, loss = 17096909.77846510\n",
            "Iteration 1462, loss = 17123119.10874559\n",
            "Iteration 1463, loss = 17067915.62739820\n",
            "Iteration 1464, loss = 17081394.11516957\n",
            "Iteration 1465, loss = 17085236.15209492\n",
            "Iteration 1466, loss = 17091328.06882932\n",
            "Iteration 1467, loss = 17079204.14651939\n",
            "Iteration 1468, loss = 17084582.46851638\n",
            "Iteration 1469, loss = 17061052.77809481\n",
            "Iteration 1470, loss = 17099687.25375313\n",
            "Iteration 1471, loss = 17041786.67916761\n",
            "Iteration 1472, loss = 17061684.58628479\n",
            "Iteration 1473, loss = 17061217.07455301\n",
            "Iteration 1474, loss = 17048250.52945109\n",
            "Iteration 1475, loss = 17048189.19006245\n",
            "Iteration 1476, loss = 17072551.47108467\n",
            "Iteration 1477, loss = 17039868.50327908\n",
            "Iteration 1478, loss = 17051792.71164969\n",
            "Iteration 1479, loss = 17080025.74590386\n",
            "Iteration 1480, loss = 17052905.26676092\n",
            "Iteration 1481, loss = 17050932.99297144\n",
            "Iteration 1482, loss = 17033395.41386887\n",
            "Iteration 1483, loss = 17031131.62305753\n",
            "Iteration 1484, loss = 17032090.91742095\n",
            "Iteration 1485, loss = 17020234.89004062\n",
            "Iteration 1486, loss = 17017165.52167517\n",
            "Iteration 1487, loss = 17020911.91792858\n",
            "Iteration 1488, loss = 17018998.35374312\n",
            "Iteration 1489, loss = 17018955.93856261\n",
            "Iteration 1490, loss = 17017699.77102932\n",
            "Iteration 1491, loss = 17006134.59564594\n",
            "Iteration 1492, loss = 17026431.17276728\n",
            "Iteration 1493, loss = 16989658.46332338\n",
            "Iteration 1494, loss = 17003621.79100388\n",
            "Iteration 1495, loss = 17011127.72063849\n",
            "Iteration 1496, loss = 17009048.70464675\n",
            "Iteration 1497, loss = 16996036.00812970\n",
            "Iteration 1498, loss = 16984129.02660473\n",
            "Iteration 1499, loss = 16995936.57717050\n",
            "Iteration 1500, loss = 16994500.05637218\n",
            "Iteration 1501, loss = 17003810.77662738\n",
            "Iteration 1502, loss = 16990828.55477995\n",
            "Iteration 1503, loss = 16978670.58008409\n",
            "Iteration 1504, loss = 16976996.08335618\n",
            "Iteration 1505, loss = 16971285.08522023\n",
            "Iteration 1506, loss = 16966931.76314330\n",
            "Iteration 1507, loss = 16967468.86369206\n",
            "Iteration 1508, loss = 16962833.05836153\n",
            "Iteration 1509, loss = 16959611.83885530\n",
            "Iteration 1510, loss = 16966246.15713248\n",
            "Iteration 1511, loss = 16953653.55154268\n",
            "Iteration 1512, loss = 16958417.84349291\n",
            "Iteration 1513, loss = 16964932.22335210\n",
            "Iteration 1514, loss = 16949110.86759714\n",
            "Iteration 1515, loss = 16950392.24737117\n",
            "Iteration 1516, loss = 16950957.33321742\n",
            "Iteration 1517, loss = 16958835.74670793\n",
            "Iteration 1518, loss = 16944213.46717608\n",
            "Iteration 1519, loss = 16944542.72169051\n",
            "Iteration 1520, loss = 16951251.54238595\n",
            "Iteration 1521, loss = 16948014.48514567\n",
            "Iteration 1522, loss = 16963233.21083980\n",
            "Iteration 1523, loss = 16924376.19120883\n",
            "Iteration 1524, loss = 16943138.11289136\n",
            "Iteration 1525, loss = 16921731.63146618\n",
            "Iteration 1526, loss = 16915382.82981134\n",
            "Iteration 1527, loss = 16935443.26553480\n",
            "Iteration 1528, loss = 16910320.53980839\n",
            "Iteration 1529, loss = 16927341.20524690\n",
            "Iteration 1530, loss = 16926174.80018113\n",
            "Iteration 1531, loss = 16920416.85716911\n",
            "Iteration 1532, loss = 16923062.17999500\n",
            "Iteration 1533, loss = 16899877.01073936\n",
            "Iteration 1534, loss = 16907509.94608667\n",
            "Iteration 1535, loss = 16896524.71803700\n",
            "Iteration 1536, loss = 16899178.78738073\n",
            "Iteration 1537, loss = 16891677.42699473\n",
            "Iteration 1538, loss = 16899482.92454568\n",
            "Iteration 1539, loss = 16895971.04392855\n",
            "Iteration 1540, loss = 16907342.70133774\n",
            "Iteration 1541, loss = 16887316.57305797\n",
            "Iteration 1542, loss = 16893491.73513148\n",
            "Iteration 1543, loss = 16880367.83502215\n",
            "Iteration 1544, loss = 16901013.52484170\n",
            "Iteration 1545, loss = 16906003.20600839\n",
            "Iteration 1546, loss = 16874901.09583544\n",
            "Iteration 1547, loss = 16866244.12070356\n",
            "Iteration 1548, loss = 16886130.40243999\n",
            "Iteration 1549, loss = 16871126.20115121\n",
            "Iteration 1550, loss = 16854051.54573829\n",
            "Iteration 1551, loss = 16859833.59922981\n",
            "Iteration 1552, loss = 16847983.25210463\n",
            "Iteration 1553, loss = 16869561.21158041\n",
            "Iteration 1554, loss = 16856813.86396399\n",
            "Iteration 1555, loss = 16860070.14381482\n",
            "Iteration 1556, loss = 16839285.32862963\n",
            "Iteration 1557, loss = 16845881.16232465\n",
            "Iteration 1558, loss = 16870983.05394599\n",
            "Iteration 1559, loss = 16839894.64238380\n",
            "Iteration 1560, loss = 16849574.62385811\n",
            "Iteration 1561, loss = 16833680.79777448\n",
            "Iteration 1562, loss = 16827682.12600510\n",
            "Iteration 1563, loss = 16833792.41370907\n",
            "Iteration 1564, loss = 16817855.76721632\n",
            "Iteration 1565, loss = 16833414.93803351\n",
            "Iteration 1566, loss = 16841752.53402673\n",
            "Iteration 1567, loss = 16810115.76334415\n",
            "Iteration 1568, loss = 16822158.98706619\n",
            "Iteration 1569, loss = 16846725.02753455\n",
            "Iteration 1570, loss = 16826165.87915177\n",
            "Iteration 1571, loss = 16803114.93533754\n",
            "Iteration 1572, loss = 16815799.49545137\n",
            "Iteration 1573, loss = 16799852.47691793\n",
            "Iteration 1574, loss = 16821182.02018461\n",
            "Iteration 1575, loss = 16842399.42710904\n",
            "Iteration 1576, loss = 16794729.06580767\n",
            "Iteration 1577, loss = 16791945.58045295\n",
            "Iteration 1578, loss = 16808069.69801969\n",
            "Iteration 1579, loss = 16799504.62109596\n",
            "Iteration 1580, loss = 16818592.37757376\n",
            "Iteration 1581, loss = 16780481.90370486\n",
            "Iteration 1582, loss = 16788312.31306153\n",
            "Iteration 1583, loss = 16782668.61656933\n",
            "Iteration 1584, loss = 16772395.39532402\n",
            "Iteration 1585, loss = 16797841.98312055\n",
            "Iteration 1586, loss = 16791083.65765022\n",
            "Iteration 1587, loss = 16774655.39857125\n",
            "Iteration 1588, loss = 16777567.94772635\n",
            "Iteration 1589, loss = 16769079.18274449\n",
            "Iteration 1590, loss = 16763496.26772643\n",
            "Iteration 1591, loss = 16751044.71444802\n",
            "Iteration 1592, loss = 16747835.56223604\n",
            "Iteration 1593, loss = 16757611.00305502\n",
            "Iteration 1594, loss = 16755685.20725871\n",
            "Iteration 1595, loss = 16751881.68919867\n",
            "Iteration 1596, loss = 16763041.76674713\n",
            "Iteration 1597, loss = 16758116.10562818\n",
            "Iteration 1598, loss = 16742445.04382296\n",
            "Iteration 1599, loss = 16735111.56255342\n",
            "Iteration 1600, loss = 16729506.42881003\n",
            "Iteration 1601, loss = 16723976.00936803\n",
            "Iteration 1602, loss = 16725354.89143043\n",
            "Iteration 1603, loss = 16723195.06489570\n",
            "Iteration 1604, loss = 16721983.66703604\n",
            "Iteration 1605, loss = 16716732.62933760\n",
            "Iteration 1606, loss = 16709599.82909163\n",
            "Iteration 1607, loss = 16722774.96337188\n",
            "Iteration 1608, loss = 16706377.98311665\n",
            "Iteration 1609, loss = 16713930.88496794\n",
            "Iteration 1610, loss = 16723853.87786253\n",
            "Iteration 1611, loss = 16701869.88114367\n",
            "Iteration 1612, loss = 16723907.11082168\n",
            "Iteration 1613, loss = 16743119.29400018\n",
            "Iteration 1614, loss = 16707411.42986304\n",
            "Iteration 1615, loss = 16697010.14659878\n",
            "Iteration 1616, loss = 16721392.57320777\n",
            "Iteration 1617, loss = 16678423.78238611\n",
            "Iteration 1618, loss = 16700936.19654829\n",
            "Iteration 1619, loss = 16699407.35200921\n",
            "Iteration 1620, loss = 16670870.66627315\n",
            "Iteration 1621, loss = 16683896.76433734\n",
            "Iteration 1622, loss = 16671798.85449767\n",
            "Iteration 1623, loss = 16665878.03298252\n",
            "Iteration 1624, loss = 16675937.81754317\n",
            "Iteration 1625, loss = 16673171.10227196\n",
            "Iteration 1626, loss = 16661552.04819148\n",
            "Iteration 1627, loss = 16666303.02145514\n",
            "Iteration 1628, loss = 16670576.24661587\n",
            "Iteration 1629, loss = 16652193.77764378\n",
            "Iteration 1630, loss = 16662962.66495793\n",
            "Iteration 1631, loss = 16653820.87979953\n",
            "Iteration 1632, loss = 16652924.51448670\n",
            "Iteration 1633, loss = 16664845.27007082\n",
            "Iteration 1634, loss = 16635705.58596528\n",
            "Iteration 1635, loss = 16646002.16914216\n",
            "Iteration 1636, loss = 16638017.10091967\n",
            "Iteration 1637, loss = 16630140.41585842\n",
            "Iteration 1638, loss = 16632792.91842732\n",
            "Iteration 1639, loss = 16641557.72829939\n",
            "Iteration 1640, loss = 16648418.29492485\n",
            "Iteration 1641, loss = 16622957.78746453\n",
            "Iteration 1642, loss = 16628510.51981967\n",
            "Iteration 1643, loss = 16675086.34000683\n",
            "Iteration 1644, loss = 16610163.59428484\n",
            "Iteration 1645, loss = 16656495.91755886\n",
            "Iteration 1646, loss = 16621818.83992157\n",
            "Iteration 1647, loss = 16667364.99153448\n",
            "Iteration 1648, loss = 16664731.78780737\n",
            "Iteration 1649, loss = 16629117.45523852\n",
            "Iteration 1650, loss = 16603013.33863901\n",
            "Iteration 1651, loss = 16599814.06064783\n",
            "Iteration 1652, loss = 16594983.74543212\n",
            "Iteration 1653, loss = 16597182.75670738\n",
            "Iteration 1654, loss = 16584388.93376621\n",
            "Iteration 1655, loss = 16604980.74626153\n",
            "Iteration 1656, loss = 16591681.26024436\n",
            "Iteration 1657, loss = 16576332.97469632\n",
            "Iteration 1658, loss = 16575601.15873674\n",
            "Iteration 1659, loss = 16574356.11625342\n",
            "Iteration 1660, loss = 16583011.30684385\n",
            "Iteration 1661, loss = 16567083.17931819\n",
            "Iteration 1662, loss = 16600900.22924503\n",
            "Iteration 1663, loss = 16569433.28976991\n",
            "Iteration 1664, loss = 16574846.60778129\n",
            "Iteration 1665, loss = 16554501.93816831\n",
            "Iteration 1666, loss = 16558844.13639569\n",
            "Iteration 1667, loss = 16558030.42064967\n",
            "Iteration 1668, loss = 16549400.25513807\n",
            "Iteration 1669, loss = 16550151.52759233\n",
            "Iteration 1670, loss = 16572946.55362750\n",
            "Iteration 1671, loss = 16545068.54718382\n",
            "Iteration 1672, loss = 16535107.85906682\n",
            "Iteration 1673, loss = 16542664.49365678\n",
            "Iteration 1674, loss = 16535738.26930770\n",
            "Iteration 1675, loss = 16538496.22998766\n",
            "Iteration 1676, loss = 16542772.25099402\n",
            "Iteration 1677, loss = 16538715.21095427\n",
            "Iteration 1678, loss = 16529916.41084259\n",
            "Iteration 1679, loss = 16529761.51455421\n",
            "Iteration 1680, loss = 16530087.62805686\n",
            "Iteration 1681, loss = 16555881.63443962\n",
            "Iteration 1682, loss = 16534563.16609396\n",
            "Iteration 1683, loss = 16517781.27411195\n",
            "Iteration 1684, loss = 16515697.69625491\n",
            "Iteration 1685, loss = 16507863.09562755\n",
            "Iteration 1686, loss = 16503106.74218084\n",
            "Iteration 1687, loss = 16501733.41586904\n",
            "Iteration 1688, loss = 16511701.99518053\n",
            "Iteration 1689, loss = 16494963.46908373\n",
            "Iteration 1690, loss = 16512114.88641247\n",
            "Iteration 1691, loss = 16509846.80719039\n",
            "Iteration 1692, loss = 16489618.03899512\n",
            "Iteration 1693, loss = 16492423.34685063\n",
            "Iteration 1694, loss = 16482260.79567786\n",
            "Iteration 1695, loss = 16493638.10254871\n",
            "Iteration 1696, loss = 16481690.63824626\n",
            "Iteration 1697, loss = 16481431.18651328\n",
            "Iteration 1698, loss = 16497807.30870850\n",
            "Iteration 1699, loss = 16478708.54516994\n",
            "Iteration 1700, loss = 16470761.75402406\n",
            "Iteration 1701, loss = 16478676.10532738\n",
            "Iteration 1702, loss = 16459398.76659019\n",
            "Iteration 1703, loss = 16481863.98898605\n",
            "Iteration 1704, loss = 16470178.64516967\n",
            "Iteration 1705, loss = 16469452.00784317\n",
            "Iteration 1706, loss = 16458308.24195088\n",
            "Iteration 1707, loss = 16488039.61829752\n",
            "Iteration 1708, loss = 16444579.24151109\n",
            "Iteration 1709, loss = 16481507.49462519\n",
            "Iteration 1710, loss = 16438098.21597805\n",
            "Iteration 1711, loss = 16469237.53277665\n",
            "Iteration 1712, loss = 16423233.53184220\n",
            "Iteration 1713, loss = 16443387.29150480\n",
            "Iteration 1714, loss = 16441970.44375880\n",
            "Iteration 1715, loss = 16426396.63736750\n",
            "Iteration 1716, loss = 16430793.80938952\n",
            "Iteration 1717, loss = 16412330.51335379\n",
            "Iteration 1718, loss = 16418572.03842412\n",
            "Iteration 1719, loss = 16414223.27752636\n",
            "Iteration 1720, loss = 16411676.56680019\n",
            "Iteration 1721, loss = 16408871.56371325\n",
            "Iteration 1722, loss = 16406087.14936086\n",
            "Iteration 1723, loss = 16408859.32638557\n",
            "Iteration 1724, loss = 16406778.12648034\n",
            "Iteration 1725, loss = 16397829.88106558\n",
            "Iteration 1726, loss = 16408970.73985154\n",
            "Iteration 1727, loss = 16390572.49317997\n",
            "Iteration 1728, loss = 16419725.57702460\n",
            "Iteration 1729, loss = 16391131.66352997\n",
            "Iteration 1730, loss = 16398620.73611177\n",
            "Iteration 1731, loss = 16399347.95366403\n",
            "Iteration 1732, loss = 16394981.49298823\n",
            "Iteration 1733, loss = 16391400.77054360\n",
            "Iteration 1734, loss = 16371157.63080198\n",
            "Iteration 1735, loss = 16482405.94770637\n",
            "Iteration 1736, loss = 16346083.60910590\n",
            "Iteration 1737, loss = 16408964.31075101\n",
            "Iteration 1738, loss = 16365040.46524163\n",
            "Iteration 1739, loss = 16390282.79673548\n",
            "Iteration 1740, loss = 16388094.60254650\n",
            "Iteration 1741, loss = 16371013.07468762\n",
            "Iteration 1742, loss = 16353537.97058653\n",
            "Iteration 1743, loss = 16414445.02255053\n",
            "Iteration 1744, loss = 16352486.69264604\n",
            "Iteration 1745, loss = 16358306.38027398\n",
            "Iteration 1746, loss = 16333116.57894052\n",
            "Iteration 1747, loss = 16362583.01353528\n",
            "Iteration 1748, loss = 16338363.35873323\n",
            "Iteration 1749, loss = 16356147.66163478\n",
            "Iteration 1750, loss = 16331106.43102263\n",
            "Iteration 1751, loss = 16491922.31482335\n",
            "Iteration 1752, loss = 16429095.20906367\n",
            "Iteration 1753, loss = 16365935.79451079\n",
            "Iteration 1754, loss = 16333722.32684893\n",
            "Iteration 1755, loss = 16335907.74950007\n",
            "Iteration 1756, loss = 16326790.03485136\n",
            "Iteration 1757, loss = 16319562.28856556\n",
            "Iteration 1758, loss = 16300440.18192795\n",
            "Iteration 1759, loss = 16315060.66777084\n",
            "Iteration 1760, loss = 16329715.05188614\n",
            "Iteration 1761, loss = 16313671.69482405\n",
            "Iteration 1762, loss = 16305738.02740247\n",
            "Iteration 1763, loss = 16300946.39659352\n",
            "Iteration 1764, loss = 16297176.21481648\n",
            "Iteration 1765, loss = 16288298.96814042\n",
            "Iteration 1766, loss = 16295846.77020246\n",
            "Iteration 1767, loss = 16319870.90197736\n",
            "Iteration 1768, loss = 16276501.83153214\n",
            "Iteration 1769, loss = 16293556.18677235\n",
            "Iteration 1770, loss = 16291476.87836411\n",
            "Iteration 1771, loss = 16280808.78521295\n",
            "Iteration 1772, loss = 16278002.19222062\n",
            "Iteration 1773, loss = 16271864.54683972\n",
            "Iteration 1774, loss = 16272480.70574305\n",
            "Iteration 1775, loss = 16260883.61276213\n",
            "Iteration 1776, loss = 16265938.48065302\n",
            "Iteration 1777, loss = 16263042.64878086\n",
            "Iteration 1778, loss = 16261013.18619714\n",
            "Iteration 1779, loss = 16269555.18351982\n",
            "Iteration 1780, loss = 16256622.29841175\n",
            "Iteration 1781, loss = 16249509.57387815\n",
            "Iteration 1782, loss = 16272830.74721563\n",
            "Iteration 1783, loss = 16249178.58413943\n",
            "Iteration 1784, loss = 16262942.71691421\n",
            "Iteration 1785, loss = 16244214.74039459\n",
            "Iteration 1786, loss = 16248738.64631327\n",
            "Iteration 1787, loss = 16226187.42314349\n",
            "Iteration 1788, loss = 16232699.59872837\n",
            "Iteration 1789, loss = 16233429.31671363\n",
            "Iteration 1790, loss = 16234119.09077461\n",
            "Iteration 1791, loss = 16222827.88353924\n",
            "Iteration 1792, loss = 16229379.54982560\n",
            "Iteration 1793, loss = 16222555.06886695\n",
            "Iteration 1794, loss = 16223419.74448303\n",
            "Iteration 1795, loss = 16233505.53180277\n",
            "Iteration 1796, loss = 16207253.68171277\n",
            "Iteration 1797, loss = 16229389.29089915\n",
            "Iteration 1798, loss = 16212045.87477113\n",
            "Iteration 1799, loss = 16253115.94360108\n",
            "Iteration 1800, loss = 16256126.52234357\n",
            "Iteration 1801, loss = 16213567.90424687\n",
            "Iteration 1802, loss = 16198023.19784966\n",
            "Iteration 1803, loss = 16204802.87809059\n",
            "Iteration 1804, loss = 16207816.23150479\n",
            "Iteration 1805, loss = 16203977.20750579\n",
            "Iteration 1806, loss = 16182693.55847647\n",
            "Iteration 1807, loss = 16194692.21651928\n",
            "Iteration 1808, loss = 16199366.79029818\n",
            "Iteration 1809, loss = 16184140.54930184\n",
            "Iteration 1810, loss = 16181624.33752219\n",
            "Iteration 1811, loss = 16193421.64259367\n",
            "Iteration 1812, loss = 16173383.90572846\n",
            "Iteration 1813, loss = 16170815.46931525\n",
            "Iteration 1814, loss = 16178485.78055117\n",
            "Iteration 1815, loss = 16158559.88206202\n",
            "Iteration 1816, loss = 16176063.61939739\n",
            "Iteration 1817, loss = 16159706.76838795\n",
            "Iteration 1818, loss = 16169436.34327794\n",
            "Iteration 1819, loss = 16151646.37431102\n",
            "Iteration 1820, loss = 16157399.42324469\n",
            "Iteration 1821, loss = 16145018.12836347\n",
            "Iteration 1822, loss = 16161532.59503808\n",
            "Iteration 1823, loss = 16145837.95032530\n",
            "Iteration 1824, loss = 16191777.97694720\n",
            "Iteration 1825, loss = 16140488.19668117\n",
            "Iteration 1826, loss = 16154627.42751801\n",
            "Iteration 1827, loss = 16149528.41928100\n",
            "Iteration 1828, loss = 16133782.77486123\n",
            "Iteration 1829, loss = 16128188.48329096\n",
            "Iteration 1830, loss = 16136030.80254875\n",
            "Iteration 1831, loss = 16122488.72889888\n",
            "Iteration 1832, loss = 16148591.17978889\n",
            "Iteration 1833, loss = 16167415.86205051\n",
            "Iteration 1834, loss = 16118826.14047236\n",
            "Iteration 1835, loss = 16117442.27210408\n",
            "Iteration 1836, loss = 16109168.25382848\n",
            "Iteration 1837, loss = 16117632.33009901\n",
            "Iteration 1838, loss = 16091881.39151022\n",
            "Iteration 1839, loss = 16127132.67223370\n",
            "Iteration 1840, loss = 16099480.27506731\n",
            "Iteration 1841, loss = 16102377.75004349\n",
            "Iteration 1842, loss = 16096056.73241089\n",
            "Iteration 1843, loss = 16127120.20759842\n",
            "Iteration 1844, loss = 16104478.90363972\n",
            "Iteration 1845, loss = 16109567.30248689\n",
            "Iteration 1846, loss = 16102827.24257018\n",
            "Iteration 1847, loss = 16114421.23862407\n",
            "Iteration 1848, loss = 16077889.26355740\n",
            "Iteration 1849, loss = 16072462.33826697\n",
            "Iteration 1850, loss = 16091876.80905345\n",
            "Iteration 1851, loss = 16082194.96138179\n",
            "Iteration 1852, loss = 16084001.15390565\n",
            "Iteration 1853, loss = 16099064.05268216\n",
            "Iteration 1854, loss = 16084340.57893070\n",
            "Iteration 1855, loss = 16053914.01407037\n",
            "Iteration 1856, loss = 16084681.83474956\n",
            "Iteration 1857, loss = 16062622.15378881\n",
            "Iteration 1858, loss = 16057840.49267668\n",
            "Iteration 1859, loss = 16048639.56299767\n",
            "Iteration 1860, loss = 16061842.05242603\n",
            "Iteration 1861, loss = 16050175.44876306\n",
            "Iteration 1862, loss = 16054620.06157713\n",
            "Iteration 1863, loss = 16038805.46110378\n",
            "Iteration 1864, loss = 16038871.05041011\n",
            "Iteration 1865, loss = 16041807.27887789\n",
            "Iteration 1866, loss = 16082179.62978178\n",
            "Iteration 1867, loss = 16041913.02442572\n",
            "Iteration 1868, loss = 16055822.76003632\n",
            "Iteration 1869, loss = 16045224.79364445\n",
            "Iteration 1870, loss = 16027519.59064390\n",
            "Iteration 1871, loss = 16017899.65485324\n",
            "Iteration 1872, loss = 16015695.04093745\n",
            "Iteration 1873, loss = 16015280.08730262\n",
            "Iteration 1874, loss = 16013432.25253110\n",
            "Iteration 1875, loss = 16029145.09106831\n",
            "Iteration 1876, loss = 16000342.90688217\n",
            "Iteration 1877, loss = 16021785.16546958\n",
            "Iteration 1878, loss = 16014564.71319577\n",
            "Iteration 1879, loss = 16009457.16277210\n",
            "Iteration 1880, loss = 15994282.85569006\n",
            "Iteration 1881, loss = 15999918.54992386\n",
            "Iteration 1882, loss = 16002115.36956608\n",
            "Iteration 1883, loss = 15989113.98389980\n",
            "Iteration 1884, loss = 15987827.48699906\n",
            "Iteration 1885, loss = 15985590.65093278\n",
            "Iteration 1886, loss = 15986420.32369087\n",
            "Iteration 1887, loss = 15982219.35647166\n",
            "Iteration 1888, loss = 15974958.16922849\n",
            "Iteration 1889, loss = 15974322.88636468\n",
            "Iteration 1890, loss = 15973872.28353503\n",
            "Iteration 1891, loss = 15983174.90368263\n",
            "Iteration 1892, loss = 15980657.50893223\n",
            "Iteration 1893, loss = 15960806.49166175\n",
            "Iteration 1894, loss = 15959231.39500131\n",
            "Iteration 1895, loss = 15973308.87233252\n",
            "Iteration 1896, loss = 15956982.13449938\n",
            "Iteration 1897, loss = 15948511.23247772\n",
            "Iteration 1898, loss = 15950189.39078321\n",
            "Iteration 1899, loss = 15960519.17048905\n",
            "Iteration 1900, loss = 15943020.60449159\n",
            "Iteration 1901, loss = 15943964.48293751\n",
            "Iteration 1902, loss = 15953095.83431904\n",
            "Iteration 1903, loss = 15944737.81915039\n",
            "Iteration 1904, loss = 15944043.57706385\n",
            "Iteration 1905, loss = 15929662.17151484\n",
            "Iteration 1906, loss = 15963348.00061292\n",
            "Iteration 1907, loss = 15937023.97099333\n",
            "Iteration 1908, loss = 15931829.69836410\n",
            "Iteration 1909, loss = 15962975.66904781\n",
            "Iteration 1910, loss = 15927252.48368779\n",
            "Iteration 1911, loss = 15941303.23729300\n",
            "Iteration 1912, loss = 15914437.46995737\n",
            "Iteration 1913, loss = 15922032.41513946\n",
            "Iteration 1914, loss = 15924545.58981700\n",
            "Iteration 1915, loss = 15916403.10168237\n",
            "Iteration 1916, loss = 15903190.87773188\n",
            "Iteration 1917, loss = 15953815.46164076\n",
            "Iteration 1918, loss = 15927677.99611067\n",
            "Iteration 1919, loss = 15942508.23253690\n",
            "Iteration 1920, loss = 15897184.51926664\n",
            "Iteration 1921, loss = 15900009.45980258\n",
            "Iteration 1922, loss = 15903524.03058042\n",
            "Iteration 1923, loss = 15888332.12894865\n",
            "Iteration 1924, loss = 15894525.62010710\n",
            "Iteration 1925, loss = 15907569.46832978\n",
            "Iteration 1926, loss = 15872659.47112146\n",
            "Iteration 1927, loss = 15882348.08220999\n",
            "Iteration 1928, loss = 15875526.78593244\n",
            "Iteration 1929, loss = 15876337.54129455\n",
            "Iteration 1930, loss = 15882125.44441716\n",
            "Iteration 1931, loss = 15892667.80890174\n",
            "Iteration 1932, loss = 15909003.58463301\n",
            "Iteration 1933, loss = 15880930.46946522\n",
            "Iteration 1934, loss = 15886368.80223669\n",
            "Iteration 1935, loss = 15854986.44588755\n",
            "Iteration 1936, loss = 15866205.58372927\n",
            "Iteration 1937, loss = 15872315.70700671\n",
            "Iteration 1938, loss = 15895927.66014523\n",
            "Iteration 1939, loss = 15852412.01139727\n",
            "Iteration 1940, loss = 15856892.49169256\n",
            "Iteration 1941, loss = 15846746.81504220\n",
            "Iteration 1942, loss = 15854845.60814576\n",
            "Iteration 1943, loss = 15843199.79558166\n",
            "Iteration 1944, loss = 15845322.93968544\n",
            "Iteration 1945, loss = 15845382.25009363\n",
            "Iteration 1946, loss = 15834626.89252288\n",
            "Iteration 1947, loss = 15828013.34830459\n",
            "Iteration 1948, loss = 15833365.77233878\n",
            "Iteration 1949, loss = 15843159.21738102\n",
            "Iteration 1950, loss = 15845806.15798644\n",
            "Iteration 1951, loss = 15832525.76588692\n",
            "Iteration 1952, loss = 15823764.36265041\n",
            "Iteration 1953, loss = 15822739.77148680\n",
            "Iteration 1954, loss = 15809877.69594125\n",
            "Iteration 1955, loss = 15859450.17169677\n",
            "Iteration 1956, loss = 15811639.14886040\n",
            "Iteration 1957, loss = 15805210.47550149\n",
            "Iteration 1958, loss = 15800268.68731236\n",
            "Iteration 1959, loss = 15824534.92393264\n",
            "Iteration 1960, loss = 15822102.74220172\n",
            "Iteration 1961, loss = 15794911.00066871\n",
            "Iteration 1962, loss = 15814390.28866694\n",
            "Iteration 1963, loss = 15799570.32847558\n",
            "Iteration 1964, loss = 15817647.25896182\n",
            "Iteration 1965, loss = 15782785.33178940\n",
            "Iteration 1966, loss = 15804333.55260110\n",
            "Iteration 1967, loss = 15775533.38687333\n",
            "Iteration 1968, loss = 15823831.01406662\n",
            "Iteration 1969, loss = 15793189.63270442\n",
            "Iteration 1970, loss = 15785642.43003330\n",
            "Iteration 1971, loss = 15770272.37691009\n",
            "Iteration 1972, loss = 15783577.52214874\n",
            "Iteration 1973, loss = 15767588.61761923\n",
            "Iteration 1974, loss = 15770720.21659273\n",
            "Iteration 1975, loss = 15763597.67033768\n",
            "Iteration 1976, loss = 15758586.92657471\n",
            "Iteration 1977, loss = 15753654.55198518\n",
            "Iteration 1978, loss = 15750255.66266677\n",
            "Iteration 1979, loss = 15764197.56952213\n",
            "Iteration 1980, loss = 15744121.09392630\n",
            "Iteration 1981, loss = 15769952.04152986\n",
            "Iteration 1982, loss = 15735189.81066033\n",
            "Iteration 1983, loss = 15757317.05188717\n",
            "Iteration 1984, loss = 15732460.82467026\n",
            "Iteration 1985, loss = 15736160.22469799\n",
            "Iteration 1986, loss = 15737473.31879850\n",
            "Iteration 1987, loss = 15728771.92435519\n",
            "Iteration 1988, loss = 15739754.76549329\n",
            "Iteration 1989, loss = 15726225.18513503\n",
            "Iteration 1990, loss = 15733577.13085513\n",
            "Iteration 1991, loss = 15731975.69879013\n",
            "Iteration 1992, loss = 15723250.00965814\n",
            "Iteration 1993, loss = 15714711.78137420\n",
            "Iteration 1994, loss = 15728045.71975581\n",
            "Iteration 1995, loss = 15736022.22880898\n",
            "Iteration 1996, loss = 15734326.23487669\n",
            "Iteration 1997, loss = 15734810.05700033\n",
            "Iteration 1998, loss = 15694807.17188880\n",
            "Iteration 1999, loss = 15704354.00225049\n",
            "Iteration 2000, loss = 15688753.44236877\n",
            "Mean squared error: 24218705.6033771\n",
            "R2 score: 0.8307341678272614\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# create and fit the model\n",
        "model = MLPRegressor(hidden_layer_sizes=(100, 100), activation='relu', solver='adam', alpha=0.0001, \n",
        "                      batch_size='auto', learning_rate='constant', learning_rate_init=0.001, \n",
        "                      shuffle=True, random_state=42, tol=0.0001, verbose=True, max_iter=2000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict the target variable using test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# evaluate the performance of the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean squared error:\", mse)\n",
        "print(\"R2 score:\", r2)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvGNu+QFyBopcnA/VFHpgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}